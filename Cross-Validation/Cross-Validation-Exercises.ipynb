{"cells": [{"cell_type": "markdown", "id": "c8d3b4bc", "metadata": {}, "source": ["# Cross-Validation Exercises"]}, {"cell_type": "code", "execution_count": 114, "id": "5d3332f5", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["'wget' is not recognized as an internal or external command,\n", "operable program or batch file.\n", "'wget' is not recognized as an internal or external command,\n", "operable program or batch file.\n"]}], "source": ["!wget https://github.com/shegenbart/Jupyter-Exercises/raw/main/data/banknote.pickle -P ../data\n", "!wget https://github.com/shegenbart/Jupyter-Exercises/raw/main/data/baknote_noisy.pickle -P ../data\n", "    \n", "    \n", "import pickle\n", "import numpy as np\n", "\n", "from dataclasses import dataclass\n", "\n", "@dataclass\n", "class BanknotesDataset:\n", "    Description: str\n", "    Attributes: list()\n", "    Targets: list()\n", "    X: np.array\n", "    Y: np.array\n", "        \n", "def load_dataset(filename):\n", "    with open(filename, 'rb') as fd:\n", "        dataset = pickle.load(fd)\n", "    return dataset\n", "\n", "dataset = load_dataset('../data/banknote_noisy.pickle')"]}, {"cell_type": "code", "execution_count": 4, "id": "d7223af8", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Noisy Version! Data were extracted from images that were taken\n", "from genuine and forged banknote-like specimens.\n", "For digitization, an industrial camera usually\n", "used for print inspection was used. The final\n", "images have 400x 400 pixels. Due to the object\n", "lens and distance to the investigated object\n", "gray-scale pictures with a resolution of about\n", "660 dpi were gained. Wavelet Transform tool were\n", "used to extract features from images.\n"]}], "source": ["print(dataset.Description)"]}, {"cell_type": "markdown", "id": "0438a813", "metadata": {}, "source": ["## Exercise 1:\n", "\n", "In this exercise we will study the effect of training a ML model using the training data. Because all our methods aim to find a model that works as good as possible on our training data, we often get an unrealistically high score on our training data. \n", "\n", "* Inspect the banknote-fraud dataset stored in ```dataset```.\n", "* Train a decision tree classifier and predict the accuracy of all data using the ```accuracy_score()``` function. \n", "* Discuss the results, is it realistic?"]}, {"cell_type": "markdown", "id": "239cc8da", "metadata": {}, "source": ["## Exercise 2:\n", "\n", "In the previous exercise we saw that using the same data to evaluate a model that was used to optimize a method gives unrealistically high scores. Instead we have to hold back on some of the data during training and use this held-back data to perform the evaluation. \n", "\n", "* Create a test-train split of the banknote-fraud dataset (```dataset```) using the ```train_test_split()``` function from sklearn. \n", "* Train another decision tree on the *train* portion and evaluate it's accuracy (```accuracy_score()```) on the *test* portion. "]}, {"cell_type": "code", "execution_count": null, "id": "2eb660e8", "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "id": "86e115ae", "metadata": {}, "source": ["## Exercise 3:\n", "\n", "There is a nice trick to know to check if your code or your validation are somewhat flawed. Shuffle your labels randomly, then train your classifier with the suffled labels. What performance would you expect from a *fair* classifier? \n", "\n", "* Shuffle your training labels (```np.random.shuffle()```)\n", "* Train a classifier, what accuracy would you expect?\n", "* Validate on your test set. "]}, {"cell_type": "code", "execution_count": null, "id": "d038a67c", "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "id": "b0bdc670", "metadata": {}, "source": ["### Exercise 4:\n", "\n", "Let's use scikit-learn's implementation of cross-cross_validate (see https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html). \n", "\n", "* Perform cross-validation using the ```cross_validate()``` function in a 10-fold cross-validation.  \n", "* Report the mean and standard deviation of your results. \n", "* Experiment with the number of folds (in k-fold cross validation), how does it influence your mean and standard deviation?"]}, {"cell_type": "code", "execution_count": null, "id": "dfcbb6e3", "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "id": "e0c090ca", "metadata": {}, "source": ["### Exercise 5:\n", "\n", "* Train a decision tree classifier for the banknotes dataset (it should already be loaded in the ```dataset``` variable), compute and visualize the confusion matrix.\n", "* Compute the accuracy, specificity, recall and precision in a 10 fold cross-validation and report the mean and standard deviation of each. \n", "\n", "**Hints:** \n", "\n", "1. Implement each measure as a function with a prototype like this:\n", "```python \n", "def accuracy(Y_pred, Y_real): \n", "        pass\n", "```\n", "\n", "2. Use a confusion matrix to compute the true-positives, true-negatives, false-positives and false-positives.\n", "3. ```cross_validate()``` accepts a parameter ```scoring```which accepts a dictionary of callables, use **lambdas** to  define your individual scorers. "]}, {"cell_type": "code", "execution_count": null, "id": "09b629d7", "metadata": {}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "id": "2ceeb19c", "metadata": {}, "outputs": [], "source": []}], "metadata": {"celltoolbar": "Edit Metadata", "kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.12"}}, "nbformat": 4, "nbformat_minor": 5}