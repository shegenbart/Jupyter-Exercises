{"cells": [{"cell_type": "markdown", "id": "c8d3b4bc", "metadata": {}, "source": ["# Cross-Validation Exercises"]}, {"cell_type": "code", "execution_count": 2, "id": "5d3332f5", "metadata": {}, "outputs": [], "source": ["#!wget https://github.com/shegenbart/Jupyter-Exercises/raw/main/data/banknote.pickle -P ../data\n", "#!wget https://github.com/shegenbart/Jupyter-Exercises/raw/main/data/baknote_noisy.pickle -P ../data\n", "    \n", "    \n", "import pickle\n", "import numpy as np\n", "\n", "from dataclasses import dataclass\n", "\n", "@dataclass\n", "class BanknotesDataset:\n", "    Description: str\n", "    Attributes: list()\n", "    Targets: list()\n", "    X: np.array\n", "    Y: np.array\n", "        \n", "def load_dataset(filename):\n", "    with open(filename, 'rb') as fd:\n", "        dataset = pickle.load(fd)\n", "    return dataset\n", "\n", "dataset = load_dataset('../data/banknote_noisy.pickle')"]}, {"cell_type": "code", "execution_count": 4, "id": "d7223af8", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Noisy Version! Data were extracted from images that were taken\n", "from genuine and forged banknote-like specimens.\n", "For digitization, an industrial camera usually\n", "used for print inspection was used. The final\n", "images have 400x 400 pixels. Due to the object\n", "lens and distance to the investigated object\n", "gray-scale pictures with a resolution of about\n", "660 dpi were gained. Wavelet Transform tool were\n", "used to extract features from images.\n"]}], "source": ["print(dataset.Description)"]}, {"cell_type": "markdown", "id": "0438a813", "metadata": {}, "source": ["## Exercise 1:\n", "\n", "In this exercise we will study the effect of training a ML model using the training data. Because all our methods aim to find a model that works as good as possible on our training data, we often get an unrealistically high score on our training data. \n", "\n", "* Inspect the banknote-fraud dataset stored in ```dataset```.\n", "* Train a decision tree classifier and predict the accuracy of all data using the ```accuracy_score()``` function. \n", "* Discuss the results, is it realistic?"]}, {"cell_type": "markdown", "id": "239cc8da", "metadata": {}, "source": ["## Exercise 2:\n", "\n", "In the previous exercise we saw that using the same data to evaluate a model that was used to optimize a method gives unrealistically high scores. Instead we have to hold back on some of the data during training and use this held-back data to perform the evaluation. \n", "\n", "* Create a test-train split of the banknote-fraud dataset (```dataset```) using the ```train_test_split()``` function from sklearn. \n", "* Train another decision tree on the *train* portion and evaluate it's accuracy (```accuracy_score()```) on the *test* portion. "]}, {"cell_type": "code", "execution_count": null, "id": "2eb660e8", "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "id": "86e115ae", "metadata": {}, "source": ["## Exercise 3:\n", "\n", "There is a nice trick to know to check if your code or your validation are somewhat flawed. Shuffle your labels randomly, then train your classifier with the suffled labels. What performance would you expect from a *fair* classifier? \n", "\n", "* Shuffle your training labels (```np.random.shuffle()```)\n", "* Train a classifier, what accuracy would you expect?\n", "* Validate on your test set. "]}, {"cell_type": "code", "execution_count": null, "id": "d15494ff", "metadata": {}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "id": "94e20273", "metadata": {}, "outputs": [], "source": []}], "metadata": {"celltoolbar": "Edit Metadata", "kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.12"}}, "nbformat": 4, "nbformat_minor": 5}