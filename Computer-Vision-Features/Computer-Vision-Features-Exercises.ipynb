{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Computer Vision Features Notebook\n", "\n", "This notebook contains exercises for the computer vision features material."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!wget https://github.com/shegenbart/Jupyter-Exercises/raw/main/data/lemur_img.png -P ../data"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Exercise 1 - Derive the min-max normalization. \n", "\n", "Derive the min-max normalization from the following constraints: \n", "\n", "i.) $f(x) := kx + d$\n", "\n", "ii.) $f(max) = 1$\n", "\n", "iii.) $f(min) = 0$\n", "\n", "\n", "*Notice:* Put your derivation in the next cell (you can either use latex notation inside markdown or a foto)."]}, {"cell_type": "markdown", "metadata": {}, "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["### Exercise 2 - Implement the min-max normalization. \n", "\n", "Use the formula you derived in the previous exercise and implement the min-max normalization for a given numpy array.  "]}, {"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [], "source": ["import numpy as np \n", "import cv2\n", "\n", "def normalize(img):\n", "    pass"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Exercise 3 - Histograms:\n", "\n", "Implement a function to compute a histogram (using a user supplied number of bins) for a given image. Notice that histograms are usually computed for each color channel separately. \n", "\n", "* Make sure your implementation can deal with normalized data (i.e. in a range of 0 and 1). \n", "* Plot your histograms of each color channel using matplotlibs ```bar()``` function. \n"]}, {"cell_type": "code", "execution_count": 246, "metadata": {}, "outputs": [], "source": ["img = cv2.imread('../data/lemur_img.png')\n", "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n", "\n", "def hist(img):\n", "    pass"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Exercise 4 - Scale Invariant Feature Transform:\n", "\n", "* Compute the SIFT descriptors for the provided images (sift1_img.png and sift2_img.png). See https://docs.opencv.org/4.x/da/df5/tutorial_py_sift_intro.html for a tutorial how to use openCV to compute SIFT descriptors. \n", "\n", "* Use an openCV feature matcher (https://docs.opencv.org/4.x/dc/dc3/tutorial_py_matcher.html) to match your descriptors and visualize the matches as shown in the slides. \n", "\n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!wget https://github.com/shegenbart/Jupyter-Exercises/raw/main/data/sift1_img.png -P ../data\n", "!wget https://github.com/shegenbart/Jupyter-Exercises/raw/main/data/sift2_img.png -P ../data"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["### Exercise 5 -  Local Binary Patterns:\n", "\n", "It is now time to develop the full computer vision pipeline to classify some images. \n", "\n", "1. Implement Local Binary Patterns (to do so use a simple 8-neighborhood).\n", "2. Write a method to compute grayscale histograms (not lbp-histograms) for all image data (X_train and X_test). Re-use your implementation from before. \n", "3. Write a method to compute lbp histograms for all image data (X_train and X_test).\n", "4. Train two separate Neural Networks and classify the data using:\n", "    * The grayscale histograms as features\n", "    * The lbp-histograms as features\n", "    \n", "Notice: You will need to one-hot-encode the labels of the dataset to train a neural network (why?). Use the ```tf.keras.utils.to_categorical(Y)``` method to encode your labels accordingly.\n", "\n", "Compare and interpret your results, what would you expect? Can you explain the results?\n", "\n", "\n", "\n", "    \n", "\n", "\n", "\n"]}, {"cell_type": "code", "execution_count": 323, "metadata": {}, "outputs": [], "source": ["import pickle, bz2\n", "from sklearn.model_selection import train_test_split\n", "\n", "!wget https://github.com/shegenbart/Jupyter-Exercises/raw/main/data/kth_tips.pbz2 -P ../data\n", "\n", "\n", "with bz2.BZ2File('../data/kth_tips.pbz2', 'rb') as fd:\n", "    kth_dataset = pickle.load(fd)\n", "\n", "X_train, X_test, Y_train, Y_test = train_test_split(kth_dataset['X'], kth_dataset['Y'], random_state=12345)\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}], "metadata": {"celltoolbar": "Edit Metadata", "kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.12"}, "vscode": {"interpreter": {"hash": "71e3996440a4286f4b5430a3d4d1ae66fa68d894e5edac3334c7ebe5f98b7546"}}}, "nbformat": 4, "nbformat_minor": 2}