{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Bayes Exercises\n", "\n", "**NOTICE:**\n", "1. You are allowed to work in groups of up to three people but **have to document** your group's\\\n", " members in the top cell of your notebook.\n", "2. **Comment your code**, explain what you do (refer to the slides). It will help you understand the topics\\\n", " and help me understand your thinking progress. Quality of comments will be graded. \n", "3. **Discuss** and analyze your results, **write-down your learnings**. These exercises are no programming\\\n", " exercises it is about learning and getting a touch for these methods. Such questions might be asked in the\\\n", " final exams. \n", "4. Feel free to **experiment** with these methods. Change parameters think about improvements, write down\\\n", " what you learned. This is not only about collecting points for the final grade, it is about understanding\\\n", "  the methods. \n", "5. All exercises can be part of the final exam, your **answers, experiments and documented learnings will be graded**. \n", "\n", "\n", "**Hints**:\n", "\n", "In many ML libraries we use the terminology ```X``` for features and ```Y``` for labels/annotations \n", "or target values. X and Y are often numpy arrays. Such that Y is a column vector and X is a multidimensional array with:\n", "* the first dimension represents each sample\n", "* the remaining index dimensions represent the feature-values (these can be multi-dimensional in case we use images)\n", "\n", "The number of samples and the number of labels (i.e.: the number of entries in the first dimension of X and Y) must be the same. \n", "Acessing the 13-th feature vector and corresponding class in our training data:\n", "```python\n", "x13 = X[12, :] # index starts with 0\n", "y13 = Y[12] \n", "```\n", "\n", "Using logical indexing could come in to be really helpful here:\n", "```python\n", "    X[Y == 0,:]  # Fetch all features (from X) according to class label 0\n", "    X[Y == 1,:]  # Fetch all features (from X) according to class label 1\n", "```\n", "\n", "Also:\n", "* ```np.unique()``` \n", "* ```np.sum()``` (have a look at the **axis** parameter)\n", "* ``` X.reshape()```\n", "\n", "could be helpful. \n"]}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [], "source": ["# This cell generates your test values (they are the same as in the decision-tree exercise).\n", "#\n", "\n", "import numpy as np\n", "# Test features from previous exercise. \n", "#\n", "X_train = np.array([\n", "                    [2, 1, 3],       # Class 1: Red, Green, Blue\n", "                    [10, 30, 20],    # Class 2: Red, Green, Blue\n", "                    [1, 3, 2],       # Class 2: Red, Green, Blue\n", "                    [40, 20, 60]     # Class 1: Red, Green, Blue\n", "                 ], dtype=\"float32\")\n", "\n", "# Labels for each feature in X_train\n", "#\n", "Y_train = np.array([0, 1, 1, 0]) "]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Exercise 1: Implementation of calculating the prior values. \n", "\n", "\n", "**Summary:** In this first exercise your job is to write a python function that is capable   \n", "of computing the priors per class.\n", "\n", "**Provided Code:** Use the method stub in the cell below for your implementation. \n", "\n", "**Your Tasks in this exercise:**\n", "1. In this exercise your job is to implement the calculation prior values for each class.\n", "2. Document your learnings. "]}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [], "source": ["def compute_priors(Y):\n", "    \"\"\" Compute the priors per class in Y. \n", "\n", "    Parameters\n", "    ----------\n", "    Y: np.array\n", "      A one dimensional numpy array containing class labels.\n", "\n", "    Returns\n", "    ----------\n", "    A dictionary containing the priors per class. \n", "\n", "    Expected Output for X_train, Y_train\n", "    ----------\n", "    {0: 0.5, 1: 0.5}\n", "    \"\"\"\n", "    pass"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Exercise 2: Implementation of calculating the pik values. \n", "\n", "**Summary:** In this exercise your job is to implement the calculation of the probability values (pik) for each feature   \n", "attribute under each class.\n", "\n", "**Provided Code:** Use the method stub in the cell below for your implementation. \n", "\n", "**Your Tasks in this exercise:**\n", "1. Implement the calculation of the pik features for each feature attribute under each class. \n", "3. Document your learnings. \n", "\n", "**Warning:**\n", "\n", "Be careful when computing the probabilities. We do not want larger images (more pixels and therefore more entries in a   \n", "non-normalized histogram) to have a larger impact. Have a look at the features, you might have to normalize the individual feature vectors. "]}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [], "source": ["def compute_pik(X, Y):\n", "   \"\"\" Compute the probabilities per feature-dimension (pik).\n", "\n", "    Parameters\n", "    ----------\n", "    X: np.array\n", "      A two dimensional numpy array containing feature vectors (in rows) for each sample\n", "      in the training data. \n", "    Y: np.array\n", "      A one dimensional numpy array containing class labels.\n", "\n", "    Returns\n", "    ----------\n", "    A dictionary containing the piks per class where keys are the class labels and values are the pik values. \n", "\n", "    Expected Output for X_train, Y_train\n", "    ----------\n", "    {0: array([0.33333333, 0.16666667, 0.5       ]),\n", "     1: array([0.16666667, 0.5       , 0.33333333])}\n", "    \"\"\"\n", "   pass"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Exercise 3: Implement the naive Bayes Classifier\n", "\n", "**Summary:** In this exercise you will use your previously implemented methods to build a naive bayes classifier.\n", "\n", "**Provided Code:** Use your own implementations done above. Use the method stub in the cell below.\n", "\n", "**Your Tasks in this exercise:**\n", "1. Implement the Bayes classifier without using the log trick.\n", "2. Implement the Bayes classifier with using the log trick. \n", "3. Compare the result with and without the log trick. Explain your findings. \n", "4. Document your learnings. \n", "\n", "**Hints:**\n", "* ```np.power()``` to compute $p_{ik}^{x_i}$ in a single instruction could be helpful\n", "* ```np.product()``` to compute the product of a list of values could be helpful"]}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [], "source": ["def classify_bayes(x, piks, priors):\n", "  \"\"\" Compute the posterior probability for a feature vector and perform classification according to naive Bayes. \n", "\n", "  Parameters\n", "  ----------\n", "  x: np.array\n", "    A one dimensional numpy array containing a single feature vector.\n", "  piks: dict\n", "    A dictionary of likelihoods as computed by compute_piks()\n", "  priors: dict\n", "    A dictionary of priors as computed by compute_priors()\n", "\n", "  Returns\n", "  ----------\n", "  Class label of the most probable class and a dictionary where the key is the class and the value is the posterior. \n", "\n", "  Expected Output for X_test[0,:]\n", "  ----------\n", "  (0, {0: 0.002057613168724279, 1: 6.430041152263372e-05})\n", "  \"\"\"\n", "  pass"]}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [], "source": ["def classify_bayes_log(x, piks, priors):\n", "  \"\"\" Compute the log-posterior probability for a feature vector and perform classification according to naive Bayes. \n", "\n", "  Parameters\n", "  ----------\n", "  x: np.array\n", "    A one dimensional numpy array containing a single feature vector.\n", "  piks: dict\n", "    A dictionary of likelihoods as computed by compute_piks()\n", "  priors: dict\n", "    A dictionary of priors as computed by compute_priors()\n", "\n", "  Returns\n", "  ----------\n", "  Class label of the most probable class and a dictionary where the key is the class and the value is the posterior. \n", "\n", "  Expected Output for X_test[0,:]\n", "  ----------\n", "  (0, {0: -6.1862086239004945, 1: -9.65194452670022})\n", "  \"\"\"\n", "  pass"]}, {"cell_type": "code", "execution_count": 8, "metadata": {}, "outputs": [], "source": ["# Use this X_test to verify your implementation. \n", "#\n", "X_test = np.array([\n", "                    [5, 0, 0],    # x1\n", "                    [2, 1, 3],    # x2\n", "                    [2, 7, 4]     # x3\n", "                 ])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}], "metadata": {"celltoolbar": "Edit Metadata", "kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.11.9"}, "vscode": {"interpreter": {"hash": "71e3996440a4286f4b5430a3d4d1ae66fa68d894e5edac3334c7ebe5f98b7546"}}}, "nbformat": 4, "nbformat_minor": 2}