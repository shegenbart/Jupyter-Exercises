{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Computer Vision CNNs Notebook\n", "\n", "This notebook contains exercises for the computer vision CNNs material."]}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["--2023-03-15 13:12:37--  https://github.com/shegenbart/Jupyter-Exercises/raw/main/data/kth_tips.pbz2\n", "Resolving github.com (github.com)... 140.82.121.4\n", "Connecting to github.com (github.com)|140.82.121.4|:443... connected.\n", "HTTP request sent, awaiting response... 302 Found\n", "Location: https://raw.githubusercontent.com/shegenbart/Jupyter-Exercises/main/data/kth_tips.pbz2 [following]\n", "--2023-03-15 13:12:37--  https://raw.githubusercontent.com/shegenbart/Jupyter-Exercises/main/data/kth_tips.pbz2\n", "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n", "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n", "HTTP request sent, awaiting response... 200 OK\n", "Length: 26821274 (26M) [application/octet-stream]\n", "Saving to: \u00e2\u20ac\u02dc../data/kth_tips.pbz2.2\u00e2\u20ac\u2122\n", "\n", "kth_tips.pbz2.2     100%[===================>]  25.58M  70.1MB/s    in 0.4s    \n", "\n", "2023-03-15 13:12:38 (70.1 MB/s) - \u00e2\u20ac\u02dc../data/kth_tips.pbz2.2\u00e2\u20ac\u2122 saved [26821274/26821274]\n", "\n"]}], "source": ["import tensorflow as tf\n", "import pickle, bz2\n", "from sklearn.model_selection import train_test_split\n", "\n", "!wget https://github.com/shegenbart/Jupyter-Exercises/raw/main/data/kth_tips.pbz2 -P ../data\n", "\n", "\n", "with bz2.BZ2File('../data/kth_tips.pbz2', 'rb') as fd:\n", "    kth_dataset = pickle.load(fd)\n", "\n", "X_train, X_test, Y_train, Y_test = train_test_split(kth_dataset['X'], kth_dataset['Y'], random_state=12345)\n", "Y_train = tf.keras.utils.to_categorical(Y_train)\n", "Y_test = tf.keras.utils.to_categorical(Y_test)\n", "\n", "X_train = X_train / 255\n", "X_test = X_test / 255"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Exercise 1 - Classify Image Dataset with Conventional Neural Network\n", "\n", "I have provided you with an image dataset (the same we used in the Local Binary Patterns exercise). Your images are in \n", "X_train, X_test while youre labels are in Y_train, Y_test. \n", "\n", "Your job is to train a neural network on the pixel values to classify the images. To do so:\n", "* Reshape the images from $(n,200,200)$ to $(n,40000)$\n", "* Create a neural network model using keras\n", "* Train your neural network on the (X_train, Y_train) and validate on the (X_test, Y_test) data. \n", "* Compare your results with the LBP-results (if you have them)\n", "* Explain what happened\n", "\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Exercise 2: Convolutional Layer\n", "\n", "In the cell below, I have provided you with a very simple CNN consisting only of a single convolutional layer. \n", "You can access the weights (kernels) of a keras model by accessing the layer with:\n", "\n", "```python \n", "kernel_list = cnn.layers[0].get_weights() # This gives us a list of kernels in the layer\n", "```\n", "\n", "\n", "Experiment with different parameters for ```filters, kernel_size``` and ```input_shape``` and have \n", "a look at the kernels to get a solid understanding of how these kernels a CNN uses look like. \n", "\n", "Answer the following questions:\n", "* How are the kernel dimensions influenced by the input dimensions?\n", "* How is the number of parameters in the network influenced by the input dimension, the kernel size and the number of filters?\n", "* Explain and summarize your findings. "]}, {"cell_type": "code", "execution_count": 59, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Model: \"sequential_20\"\n", "_________________________________________________________________\n", " Layer (type)                Output Shape              Param #   \n", "=================================================================\n", " conv2d_26 (Conv2D)          (None, 124, 124, 1)       76        \n", "                                                                 \n", " conv2d_27 (Conv2D)          (None, 122, 122, 2)       20        \n", "                                                                 \n", "=================================================================\n", "Total params: 96\n", "Trainable params: 96\n", "Non-trainable params: 0\n", "_________________________________________________________________\n"]}], "source": ["from tensorflow.keras import Sequential\n", "from tensorflow.keras.layers import Conv2D, InputLayer\n", "\n", "\n", "\n", "cnn = Sequential()\n", "cnn.add(InputLayer(input_shape=(128,128,3)))\n", "cnn.add(Conv2D(filters=1, kernel_size=(5,5)))\n", "cnn.add(Conv2D(filters=2, kernel_size=(3,3)))\n", "\n", "cnn.summary()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Exercise 3 - Pooling Layer\n", "\n", "Create a convolutional neural network with two convolution layers followed by pooling layers (use either max pooling or average pooling).\n", "\n", "Experiment with different values for ```pool_size, strides``` and study how the output shape is affected. \n", "Answer the following questions:\n", "* What is the difference in output size between max pooling and average pooling? Why?\n", "* What is the impact on the receptive field of a convolutional layer following a pooling layer?\n", "* How is the amount of parameters affected by a pooling layer?\n", " "]}, {"cell_type": "code", "execution_count": 124, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Model: \"sequential_44\"\n", "_________________________________________________________________\n", " Layer (type)                Output Shape              Param #   \n", "=================================================================\n", " conv2d_71 (Conv2D)          (None, 3, 3, 4)           16        \n", "                                                                 \n", " flatten_14 (Flatten)        (None, 36)                0         \n", "                                                                 \n", " dense_11 (Dense)            (None, 10)                360       \n", "                                                                 \n", "=================================================================\n", "Total params: 376\n", "Trainable params: 376\n", "Non-trainable params: 0\n", "_________________________________________________________________\n"]}], "source": ["from tensorflow.keras import Sequential\n", "from tensorflow.keras.layers import Conv2D, InputLayer, MaxPooling2D, AveragePooling2D, Flatten, Dense\n", "\n", "\n", "cnn = Sequential()\n", "cnn.add(InputLayer(input_shape=(4,4,1)))\n", "cnn.add(Conv2D(filters=4, kernel_size=(2,2), use_bias=False))\n", "cnn.add(Flatten())\n", "cnn.add(Dense(10, use_bias=False))\n", "cnn.summary()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Exercise 4: Flatten and GlobalAveragePooling\n", "\n", "In this exercise we will study two methods we can use to transfrom multi-dimensional feature maps to one-dimensional feature maps as used as input for Dense layers. \n", "\n", "1. Create a convolution neural network using two convolutional layers. \n", "    * Use a Flatten layer to transform your multi-dimensional feature map to a one-dimensional feature map. \n", "    * Add a Dense output layer for a classification problem with 10 classes.\n", "    * Where is the majority of the weights in your network architecture?\n", "    * How could this influence the training of your network?\n", "\n", "2. Create a convolution neural network using two convolutional layers. \n", "    * Use a GlobalAveragePooling2D layer to transform your multi-dimensional feature map to a one-dimensional feature map. \n", "    * Create a network architecture for a classification problem with 10 classes without using a Dense layer. \n", "    * Where is the majority of weights in your network architecture?\n", "    * How could this influence the training of your network?\n", "\n", "    \n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Solution 4"]}, {"cell_type": "code", "execution_count": 59, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Model: \"sequential_23\"\n", "_________________________________________________________________\n", " Layer (type)                Output Shape              Param #   \n", "=================================================================\n", " conv2d_79 (Conv2D)          (None, 124, 124, 1)       76        \n", "                                                                 \n", " max_pooling2d_61 (MaxPoolin  (None, 62, 62, 1)        0         \n", " g2D)                                                            \n", "                                                                 \n", " conv2d_80 (Conv2D)          (None, 60, 60, 2)         20        \n", "                                                                 \n", " max_pooling2d_62 (MaxPoolin  (None, 30, 30, 2)        0         \n", " g2D)                                                            \n", "                                                                 \n", " flatten_1 (Flatten)         (None, 1800)              0         \n", "                                                                 \n", " dense_33 (Dense)            (None, 10)                18010     \n", "                                                                 \n", "=================================================================\n", "Total params: 18,106\n", "Trainable params: 18,106\n", "Non-trainable params: 0\n", "_________________________________________________________________\n"]}], "source": ["\n", "from tensorflow.keras import Sequential\n", "from tensorflow.keras.layers import Conv2D, InputLayer, MaxPooling2D, AveragePooling2D\n", "\n", "\n", "cnn = Sequential()\n", "cnn.add(InputLayer(input_shape=(128,128,3)))\n", "cnn.add(Conv2D(filters=1, kernel_size=(5,5)))\n", "cnn.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n", "cnn.add(Conv2D(filters=2, kernel_size=(3,3)))\n", "cnn.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n", "cnn.add(Flatten())\n", "cnn.add(Dense(10, activation=\"Softmax\"))\n", "cnn.summary()\n"]}, {"cell_type": "code", "execution_count": 60, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Model: \"sequential_24\"\n", "_________________________________________________________________\n", " Layer (type)                Output Shape              Param #   \n", "=================================================================\n", " conv2d_81 (Conv2D)          (None, 124, 124, 32)      2432      \n", "                                                                 \n", " max_pooling2d_63 (MaxPoolin  (None, 62, 62, 32)       0         \n", " g2D)                                                            \n", "                                                                 \n", " conv2d_82 (Conv2D)          (None, 60, 60, 64)        18496     \n", "                                                                 \n", " max_pooling2d_64 (MaxPoolin  (None, 30, 30, 64)       0         \n", " g2D)                                                            \n", "                                                                 \n", " conv2d_83 (Conv2D)          (None, 28, 28, 10)        5770      \n", "                                                                 \n", " global_average_pooling2d_17  (None, 10)               0         \n", "  (GlobalAveragePooling2D)                                       \n", "                                                                 \n", " softmax_3 (Softmax)         (None, 10)                0         \n", "                                                                 \n", "=================================================================\n", "Total params: 26,698\n", "Trainable params: 26,698\n", "Non-trainable params: 0\n", "_________________________________________________________________\n"]}], "source": ["from tensorflow.keras import Sequential\n", "from tensorflow.keras.layers import Conv2D, InputLayer, MaxPooling2D, GlobalAveragePooling2D, Softmax\n", "\n", "\n", "cnn = Sequential()\n", "cnn.add(InputLayer(input_shape=(128,128,3)))\n", "cnn.add(Conv2D(filters=32, kernel_size=(5,5)))\n", "cnn.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n", "cnn.add(Conv2D(filters=64, kernel_size=(3,3)))\n", "cnn.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n", "cnn.add(Conv2D(filters=10, kernel_size=(3,3)))\n", "cnn.add(GlobalAveragePooling2D())\n", "cnn.add(Softmax())\n", "\n", "cnn.summary()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Exercise 5: Classification using CNNS\n", "\n", "\n", "In the first exercise in this notebook we already downloaded and prepared the KTH-TIPS dataset we used previously for classification. We saw that using a conventional NN\n", "did not work very well in this scenario. \n", "\n", "1. Create a CNN of your choice to train a classifier for the provided dataset. \n", "    * Train on (X_train, Y_train), Validate on (X_test, Y_test) (you can specify validation_data in the ```model.fit()``` function of keras)\n", "2. Compare using Flatten() and GlobalAveragePooling() for feeding your features into Dense layers\n", "    * How does it effect the number of parameters, the accuracy of your results and the time used to train the models?\n", "3. What is the highest accuracy you can reach with the smallest amount of parameters? \n", "\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Solution 5:"]}, {"cell_type": "code", "execution_count": 40, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Epoch 1/50\n", "10/10 [==============================] - 1s 62ms/step - loss: 2.4418 - acc: 0.1087 - val_loss: 2.0925 - val_acc: 0.1626\n", "Epoch 2/50\n", "10/10 [==============================] - 0s 49ms/step - loss: 2.0446 - acc: 0.2471 - val_loss: 1.9477 - val_acc: 0.2463\n", "Epoch 3/50\n", "10/10 [==============================] - 0s 49ms/step - loss: 1.8851 - acc: 0.2949 - val_loss: 1.7214 - val_acc: 0.3448\n", "Epoch 4/50\n", "10/10 [==============================] - 1s 52ms/step - loss: 1.6951 - acc: 0.3970 - val_loss: 1.5984 - val_acc: 0.3645\n", "Epoch 5/50\n", "10/10 [==============================] - 0s 50ms/step - loss: 1.6016 - acc: 0.4119 - val_loss: 1.5307 - val_acc: 0.3892\n", "Epoch 6/50\n", "10/10 [==============================] - 0s 50ms/step - loss: 1.5380 - acc: 0.4217 - val_loss: 1.4307 - val_acc: 0.4877\n", "Epoch 7/50\n", "10/10 [==============================] - 0s 50ms/step - loss: 1.4523 - acc: 0.4481 - val_loss: 1.4070 - val_acc: 0.4926\n", "Epoch 8/50\n", "10/10 [==============================] - 0s 50ms/step - loss: 1.3354 - acc: 0.4909 - val_loss: 1.3695 - val_acc: 0.4335\n", "Epoch 9/50\n", "10/10 [==============================] - 0s 50ms/step - loss: 1.2928 - acc: 0.5371 - val_loss: 1.3037 - val_acc: 0.4729\n", "Epoch 10/50\n", "10/10 [==============================] - 0s 49ms/step - loss: 1.2325 - acc: 0.5222 - val_loss: 1.2867 - val_acc: 0.5419\n", "Epoch 11/50\n", "10/10 [==============================] - 0s 50ms/step - loss: 1.1420 - acc: 0.6030 - val_loss: 1.1537 - val_acc: 0.6108\n", "Epoch 12/50\n", "10/10 [==============================] - 0s 50ms/step - loss: 1.0511 - acc: 0.6507 - val_loss: 1.0932 - val_acc: 0.6256\n", "Epoch 13/50\n", "10/10 [==============================] - 0s 50ms/step - loss: 0.9641 - acc: 0.6705 - val_loss: 1.1639 - val_acc: 0.5813\n", "Epoch 14/50\n", "10/10 [==============================] - 0s 49ms/step - loss: 0.8938 - acc: 0.7133 - val_loss: 1.2296 - val_acc: 0.5369\n", "Epoch 15/50\n", "10/10 [==============================] - 0s 48ms/step - loss: 0.9267 - acc: 0.6837 - val_loss: 1.3378 - val_acc: 0.6108\n", "Epoch 16/50\n", "10/10 [==============================] - 0s 49ms/step - loss: 0.8608 - acc: 0.7133 - val_loss: 0.9648 - val_acc: 0.7094\n", "Epoch 17/50\n", "10/10 [==============================] - 0s 49ms/step - loss: 0.7685 - acc: 0.7397 - val_loss: 0.8742 - val_acc: 0.7192\n", "Epoch 18/50\n", "10/10 [==============================] - 0s 48ms/step - loss: 0.6505 - acc: 0.7908 - val_loss: 0.7909 - val_acc: 0.7635\n", "Epoch 19/50\n", "10/10 [==============================] - 0s 47ms/step - loss: 0.5803 - acc: 0.8320 - val_loss: 0.7328 - val_acc: 0.7783\n", "Epoch 20/50\n", "10/10 [==============================] - 0s 48ms/step - loss: 0.5160 - acc: 0.8534 - val_loss: 0.8275 - val_acc: 0.7586\n", "Epoch 21/50\n", "10/10 [==============================] - 0s 49ms/step - loss: 0.4960 - acc: 0.8451 - val_loss: 0.7998 - val_acc: 0.7488\n", "Epoch 22/50\n", "10/10 [==============================] - 0s 49ms/step - loss: 0.4797 - acc: 0.8517 - val_loss: 0.9128 - val_acc: 0.7340\n", "Epoch 23/50\n", "10/10 [==============================] - 0s 48ms/step - loss: 0.4215 - acc: 0.8699 - val_loss: 0.6116 - val_acc: 0.8030\n", "Epoch 24/50\n", "10/10 [==============================] - 0s 48ms/step - loss: 0.3759 - acc: 0.8830 - val_loss: 0.7479 - val_acc: 0.7438\n", "Epoch 25/50\n", "10/10 [==============================] - 0s 48ms/step - loss: 0.3807 - acc: 0.8731 - val_loss: 0.5665 - val_acc: 0.8079\n", "Epoch 26/50\n", "10/10 [==============================] - 0s 49ms/step - loss: 0.4204 - acc: 0.8715 - val_loss: 0.6568 - val_acc: 0.7783\n", "Epoch 27/50\n", "10/10 [==============================] - 0s 48ms/step - loss: 0.3714 - acc: 0.8979 - val_loss: 0.5929 - val_acc: 0.8128\n", "Epoch 28/50\n", "10/10 [==============================] - 0s 50ms/step - loss: 0.3466 - acc: 0.9127 - val_loss: 0.5942 - val_acc: 0.8621\n", "Epoch 29/50\n", "10/10 [==============================] - 0s 48ms/step - loss: 0.2983 - acc: 0.9176 - val_loss: 1.0507 - val_acc: 0.7192\n", "Epoch 30/50\n", "10/10 [==============================] - 0s 48ms/step - loss: 0.3492 - acc: 0.8781 - val_loss: 0.5226 - val_acc: 0.8374\n", "Epoch 31/50\n", "10/10 [==============================] - 0s 48ms/step - loss: 0.2380 - acc: 0.9308 - val_loss: 0.4903 - val_acc: 0.8424\n", "Epoch 32/50\n", "10/10 [==============================] - 0s 49ms/step - loss: 0.2215 - acc: 0.9341 - val_loss: 0.4806 - val_acc: 0.8325\n", "Epoch 33/50\n", "10/10 [==============================] - 0s 50ms/step - loss: 0.1655 - acc: 0.9671 - val_loss: 0.5119 - val_acc: 0.8670\n", "Epoch 34/50\n", "10/10 [==============================] - 0s 47ms/step - loss: 0.1674 - acc: 0.9654 - val_loss: 0.5466 - val_acc: 0.8276\n", "Epoch 35/50\n", "10/10 [==============================] - 0s 50ms/step - loss: 0.1955 - acc: 0.9357 - val_loss: 0.4935 - val_acc: 0.8916\n", "Epoch 36/50\n", "10/10 [==============================] - 0s 48ms/step - loss: 0.1682 - acc: 0.9522 - val_loss: 0.5153 - val_acc: 0.8621\n", "Epoch 37/50\n", "10/10 [==============================] - 0s 48ms/step - loss: 0.1776 - acc: 0.9555 - val_loss: 0.5279 - val_acc: 0.8424\n", "Epoch 38/50\n", "10/10 [==============================] - 0s 48ms/step - loss: 0.2098 - acc: 0.9226 - val_loss: 0.4169 - val_acc: 0.8768\n", "Epoch 39/50\n", "10/10 [==============================] - 0s 48ms/step - loss: 0.1598 - acc: 0.9506 - val_loss: 0.5628 - val_acc: 0.8227\n", "Epoch 40/50\n", "10/10 [==============================] - 0s 48ms/step - loss: 0.1391 - acc: 0.9687 - val_loss: 0.4115 - val_acc: 0.8571\n", "Epoch 41/50\n", "10/10 [==============================] - 0s 48ms/step - loss: 0.1380 - acc: 0.9605 - val_loss: 0.4832 - val_acc: 0.8424\n", "Epoch 42/50\n", "10/10 [==============================] - 0s 48ms/step - loss: 0.1011 - acc: 0.9802 - val_loss: 0.3614 - val_acc: 0.9113\n", "Epoch 43/50\n", "10/10 [==============================] - 0s 48ms/step - loss: 0.0911 - acc: 0.9753 - val_loss: 0.3962 - val_acc: 0.8719\n", "Epoch 44/50\n", "10/10 [==============================] - 0s 48ms/step - loss: 0.0828 - acc: 0.9819 - val_loss: 0.3118 - val_acc: 0.9113\n", "Epoch 45/50\n", "10/10 [==============================] - 0s 47ms/step - loss: 0.0740 - acc: 0.9835 - val_loss: 0.3868 - val_acc: 0.8867\n", "Epoch 46/50\n", "10/10 [==============================] - 0s 47ms/step - loss: 0.0651 - acc: 0.9918 - val_loss: 0.3672 - val_acc: 0.9163\n", "Epoch 47/50\n", "10/10 [==============================] - 0s 46ms/step - loss: 0.0518 - acc: 0.9934 - val_loss: 0.3774 - val_acc: 0.9113\n", "Epoch 48/50\n", "10/10 [==============================] - 0s 47ms/step - loss: 0.0521 - acc: 0.9901 - val_loss: 0.3931 - val_acc: 0.8966\n", "Epoch 49/50\n", "10/10 [==============================] - 0s 47ms/step - loss: 0.0439 - acc: 0.9901 - val_loss: 0.3736 - val_acc: 0.8867\n", "Epoch 50/50\n", "10/10 [==============================] - 0s 50ms/step - loss: 0.0453 - acc: 0.9918 - val_loss: 0.3563 - val_acc: 0.9015\n"]}, {"data": {"text/plain": ["<keras.callbacks.History at 0x7fa1cc695c90>"]}, "execution_count": 40, "metadata": {}, "output_type": "execute_result"}], "source": ["from tensorflow.keras.layers import Conv2D, Flatten, GlobalAveragePooling2D, MaxPooling2D\n", "\n", "nclasses = 10\n", "cnn_flatten = Sequential()\n", "cnn_flatten.add(Conv2D(filters=16, kernel_size=(3,3),  input_shape=(200,200,1), use_bias=True, activation='relu'))\n", "cnn_flatten.add(MaxPooling2D(pool_size=(3,3)))\n", "cnn_flatten.add(Conv2D(filters=32,  kernel_size=(3,3),  use_bias=True, activation='relu'))\n", "cnn_flatten.add(MaxPooling2D(pool_size=(3,3)))\n", "cnn_flatten.add(Conv2D(filters=32, kernel_size=(3,3), use_bias=True, activation='relu'))\n", "cnn_flatten.add(Flatten())\n", "cnn_flatten.add(Dense(256, activation='relu'))\n", "cnn_flatten.add(Dense(nclasses, use_bias=False, activation='softmax'))\n", "\n", "opt = Adam(lr=0.001)\n", "cnn_flatten.compile(opt, loss='categorical_crossentropy', metrics=['acc'])\n", "cnn_flatten.fit(X_train, Y_train, batch_size=64, epochs=50, validation_data=(X_test, Y_test))\n", "\n", "#Epoch 50/50\n", "#10/10 [==============================] - 0s 50ms/step - loss: 0.0453 - acc: 0.9918 - val_loss: 0.3563 - val_acc: 0.9015"]}, {"cell_type": "code", "execution_count": 41, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Model: \"sequential_16\"\n", "_________________________________________________________________\n", " Layer (type)                Output Shape              Param #   \n", "=================================================================\n", " conv2d_55 (Conv2D)          (None, 198, 198, 16)      160       \n", "                                                                 \n", " max_pooling2d_46 (MaxPoolin  (None, 66, 66, 16)       0         \n", " g2D)                                                            \n", "                                                                 \n", " conv2d_56 (Conv2D)          (None, 64, 64, 32)        4640      \n", "                                                                 \n", " max_pooling2d_47 (MaxPoolin  (None, 21, 21, 32)       0         \n", " g2D)                                                            \n", "                                                                 \n", " conv2d_57 (Conv2D)          (None, 19, 19, 32)        9248      \n", "                                                                 \n", " flatten (Flatten)           (None, 11552)             0         \n", "                                                                 \n", " dense_25 (Dense)            (None, 256)               2957568   \n", "                                                                 \n", " dense_26 (Dense)            (None, 10)                2560      \n", "                                                                 \n", "=================================================================\n", "Total params: 2,974,176\n", "Trainable params: 2,974,176\n", "Non-trainable params: 0\n", "_________________________________________________________________\n"]}], "source": ["cnn_flatten.summary()"]}, {"cell_type": "code", "execution_count": 42, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Model: \"sequential_17\"\n", "_________________________________________________________________\n", " Layer (type)                Output Shape              Param #   \n", "=================================================================\n", " conv2d_58 (Conv2D)          (None, 198, 198, 16)      160       \n", "                                                                 \n", " max_pooling2d_48 (MaxPoolin  (None, 66, 66, 16)       0         \n", " g2D)                                                            \n", "                                                                 \n", " conv2d_59 (Conv2D)          (None, 64, 64, 32)        4640      \n", "                                                                 \n", " max_pooling2d_49 (MaxPoolin  (None, 21, 21, 32)       0         \n", " g2D)                                                            \n", "                                                                 \n", " conv2d_60 (Conv2D)          (None, 19, 19, 32)        9248      \n", "                                                                 \n", " max_pooling2d_50 (MaxPoolin  (None, 6, 6, 32)         0         \n", " g2D)                                                            \n", "                                                                 \n", " global_average_pooling2d_16  (None, 32)               0         \n", "  (GlobalAveragePooling2D)                                       \n", "                                                                 \n", " dense_27 (Dense)            (None, 256)               8448      \n", "                                                                 \n", " dense_28 (Dense)            (None, 10)                2560      \n", "                                                                 \n", "=================================================================\n", "Total params: 25,056\n", "Trainable params: 25,056\n", "Non-trainable params: 0\n", "_________________________________________________________________\n"]}], "source": ["from tensorflow.keras import Sequential\n", "from tensorflow.keras.layers import Conv2D, InputLayer, MaxPooling2D, GlobalAveragePooling2D, Softmax, Dense\n", "from tensorflow.keras.optimizers import Adam\n", "\n", "cnn_gap = Sequential()\n", "cnn_gap.add(InputLayer(input_shape=(200,200,1)))\n", "cnn_gap.add(Conv2D(filters=16, kernel_size=(3,3),  input_shape=(200,200,1), use_bias=True, activation='relu'))\n", "cnn_gap.add(MaxPooling2D(pool_size=(3,3)))\n", "cnn_gap.add(Conv2D(filters=32,  kernel_size=(3,3),  use_bias=True, activation='relu'))\n", "cnn_gap.add(MaxPooling2D(pool_size=(3,3)))\n", "cnn_gap.add(Conv2D(filters=32, kernel_size=(3,3), use_bias=True, activation='relu'))\n", "cnn_gap.add(MaxPooling2D(pool_size=(3,3)))\n", "cnn_gap.add(GlobalAveragePooling2D())\n", "cnn_gap.add(Dense(256, activation='relu'))\n", "cnn_gap.add(Dense(10, use_bias=False, activation='softmax'))\n", "\n", "opt = Adam(lr=0.001)\n", "cnn_gap.compile(opt, loss='categorical_crossentropy', metrics=['acc'])\n", "\n", "cnn_gap.summary()\n", "#\n", "#Epoch 194/500\n", "#10/10 [==============================] - 1s 52ms/step - loss: 0.1965 - acc: 0.9308 - val_loss: 0.3522 - val_acc: 0.8916"]}, {"cell_type": "code", "execution_count": 43, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Epoch 1/500\n", "10/10 [==============================] - 1s 63ms/step - loss: 2.2889 - acc: 0.1021 - val_loss: 2.2501 - val_acc: 0.1084\n", "Epoch 2/500\n", "10/10 [==============================] - 0s 49ms/step - loss: 2.2260 - acc: 0.1087 - val_loss: 2.1413 - val_acc: 0.1823\n", "Epoch 3/500\n", "10/10 [==============================] - 0s 49ms/step - loss: 2.1221 - acc: 0.1713 - val_loss: 2.0464 - val_acc: 0.2611\n", "Epoch 4/500\n", "10/10 [==============================] - 0s 49ms/step - loss: 2.0360 - acc: 0.2504 - val_loss: 1.9642 - val_acc: 0.2167\n", "Epoch 5/500\n", "10/10 [==============================] - 0s 50ms/step - loss: 1.9283 - acc: 0.2735 - val_loss: 1.8620 - val_acc: 0.2414\n", "Epoch 6/500\n", "10/10 [==============================] - 0s 50ms/step - loss: 1.8401 - acc: 0.2883 - val_loss: 1.7955 - val_acc: 0.2759\n", "Epoch 7/500\n", "10/10 [==============================] - 0s 50ms/step - loss: 1.8199 - acc: 0.3081 - val_loss: 1.7408 - val_acc: 0.3054\n", "Epoch 8/500\n", "10/10 [==============================] - 1s 51ms/step - loss: 1.7867 - acc: 0.2965 - val_loss: 1.7227 - val_acc: 0.3005\n", "Epoch 9/500\n", "10/10 [==============================] - 1s 51ms/step - loss: 1.7431 - acc: 0.3064 - val_loss: 1.7288 - val_acc: 0.2808\n", "Epoch 10/500\n", "10/10 [==============================] - 0s 50ms/step - loss: 1.6918 - acc: 0.3394 - val_loss: 1.6445 - val_acc: 0.3645\n", "Epoch 11/500\n", "10/10 [==============================] - 0s 48ms/step - loss: 1.6774 - acc: 0.3542 - val_loss: 1.6128 - val_acc: 0.3350\n", "Epoch 12/500\n", "10/10 [==============================] - 0s 48ms/step - loss: 1.6531 - acc: 0.3575 - val_loss: 1.7837 - val_acc: 0.2759\n", "Epoch 13/500\n", "10/10 [==============================] - 0s 46ms/step - loss: 1.6661 - acc: 0.3493 - val_loss: 1.5968 - val_acc: 0.3498\n", "Epoch 14/500\n", "10/10 [==============================] - 0s 46ms/step - loss: 1.6304 - acc: 0.3839 - val_loss: 1.5984 - val_acc: 0.3251\n", "Epoch 15/500\n", "10/10 [==============================] - 0s 47ms/step - loss: 1.5774 - acc: 0.3756 - val_loss: 1.5395 - val_acc: 0.3842\n", "Epoch 16/500\n", "10/10 [==============================] - 0s 46ms/step - loss: 1.5203 - acc: 0.4053 - val_loss: 1.4983 - val_acc: 0.4335\n", "Epoch 17/500\n", "10/10 [==============================] - 0s 46ms/step - loss: 1.4862 - acc: 0.4250 - val_loss: 1.4740 - val_acc: 0.4089\n", "Epoch 18/500\n", "10/10 [==============================] - 0s 46ms/step - loss: 1.4531 - acc: 0.4119 - val_loss: 1.5485 - val_acc: 0.3941\n", "Epoch 19/500\n", "10/10 [==============================] - 0s 47ms/step - loss: 1.4660 - acc: 0.4498 - val_loss: 1.4055 - val_acc: 0.4581\n", "Epoch 20/500\n", "10/10 [==============================] - 0s 46ms/step - loss: 1.4636 - acc: 0.4283 - val_loss: 1.4470 - val_acc: 0.3448\n", "Epoch 21/500\n", "10/10 [==============================] - 0s 48ms/step - loss: 1.4671 - acc: 0.4234 - val_loss: 1.3983 - val_acc: 0.4975\n", "Epoch 22/500\n", "10/10 [==============================] - 0s 47ms/step - loss: 1.3982 - acc: 0.4728 - val_loss: 1.3779 - val_acc: 0.4236\n", "Epoch 23/500\n", "10/10 [==============================] - 0s 47ms/step - loss: 1.4223 - acc: 0.4399 - val_loss: 1.3910 - val_acc: 0.4384\n", "Epoch 24/500\n", "10/10 [==============================] - 0s 47ms/step - loss: 1.3914 - acc: 0.4465 - val_loss: 1.3734 - val_acc: 0.4187\n", "Epoch 25/500\n", "10/10 [==============================] - 0s 47ms/step - loss: 1.3315 - acc: 0.4975 - val_loss: 1.3745 - val_acc: 0.4926\n", "Epoch 26/500\n", "10/10 [==============================] - 0s 47ms/step - loss: 1.2784 - acc: 0.5008 - val_loss: 1.3986 - val_acc: 0.4926\n", "Epoch 27/500\n", "10/10 [==============================] - 0s 47ms/step - loss: 1.2864 - acc: 0.5255 - val_loss: 1.3115 - val_acc: 0.4680\n", "Epoch 28/500\n", "10/10 [==============================] - 0s 47ms/step - loss: 1.2626 - acc: 0.5288 - val_loss: 1.3339 - val_acc: 0.5369\n", "Epoch 29/500\n", "10/10 [==============================] - 0s 47ms/step - loss: 1.2300 - acc: 0.5437 - val_loss: 1.2629 - val_acc: 0.5468\n", "Epoch 30/500\n", "10/10 [==============================] - 0s 47ms/step - loss: 1.2491 - acc: 0.5420 - val_loss: 1.2531 - val_acc: 0.5320\n", "Epoch 31/500\n", "10/10 [==============================] - 0s 47ms/step - loss: 1.2603 - acc: 0.5140 - val_loss: 1.3161 - val_acc: 0.5025\n", "Epoch 32/500\n", "10/10 [==============================] - 0s 48ms/step - loss: 1.2228 - acc: 0.5387 - val_loss: 1.2318 - val_acc: 0.5862\n", "Epoch 33/500\n", "10/10 [==============================] - 1s 53ms/step - loss: 1.1917 - acc: 0.5502 - val_loss: 1.2291 - val_acc: 0.5517\n", "Epoch 34/500\n", "10/10 [==============================] - 1s 52ms/step - loss: 1.1810 - acc: 0.5733 - val_loss: 1.3909 - val_acc: 0.5025\n", "Epoch 35/500\n", "10/10 [==============================] - 1s 51ms/step - loss: 1.2016 - acc: 0.5371 - val_loss: 1.2010 - val_acc: 0.5665\n", "Epoch 36/500\n", "10/10 [==============================] - 1s 52ms/step - loss: 1.1942 - acc: 0.5437 - val_loss: 1.1710 - val_acc: 0.6010\n", "Epoch 37/500\n", "10/10 [==============================] - 0s 49ms/step - loss: 1.1289 - acc: 0.5898 - val_loss: 1.1714 - val_acc: 0.5714\n", "Epoch 38/500\n", "10/10 [==============================] - 0s 48ms/step - loss: 1.1217 - acc: 0.5717 - val_loss: 1.1901 - val_acc: 0.5616\n", "Epoch 39/500\n", "10/10 [==============================] - 0s 48ms/step - loss: 1.1268 - acc: 0.6013 - val_loss: 1.1584 - val_acc: 0.5517\n", "Epoch 40/500\n", "10/10 [==============================] - 0s 47ms/step - loss: 1.1118 - acc: 0.5684 - val_loss: 1.1579 - val_acc: 0.6256\n", "Epoch 41/500\n", "10/10 [==============================] - 0s 48ms/step - loss: 1.0678 - acc: 0.6376 - val_loss: 1.1015 - val_acc: 0.6207\n", "Epoch 42/500\n", "10/10 [==============================] - 0s 48ms/step - loss: 1.0979 - acc: 0.5832 - val_loss: 1.0861 - val_acc: 0.6552\n", "Epoch 43/500\n", "10/10 [==============================] - 0s 49ms/step - loss: 1.0641 - acc: 0.6211 - val_loss: 1.0713 - val_acc: 0.6256\n", "Epoch 44/500\n", "10/10 [==============================] - 0s 49ms/step - loss: 1.0802 - acc: 0.5980 - val_loss: 1.0807 - val_acc: 0.6355\n", "Epoch 45/500\n", "10/10 [==============================] - 0s 51ms/step - loss: 1.0085 - acc: 0.6458 - val_loss: 1.0759 - val_acc: 0.6305\n", "Epoch 46/500\n", "10/10 [==============================] - 1s 53ms/step - loss: 1.0291 - acc: 0.6392 - val_loss: 1.1107 - val_acc: 0.6355\n", "Epoch 47/500\n", "10/10 [==============================] - 1s 54ms/step - loss: 1.0323 - acc: 0.5964 - val_loss: 1.1150 - val_acc: 0.6010\n", "Epoch 48/500\n", "10/10 [==============================] - 1s 53ms/step - loss: 1.0137 - acc: 0.6277 - val_loss: 1.0141 - val_acc: 0.6650\n", "Epoch 49/500\n", "10/10 [==============================] - 1s 54ms/step - loss: 0.9880 - acc: 0.6310 - val_loss: 1.1650 - val_acc: 0.5517\n", "Epoch 50/500\n", "10/10 [==============================] - 1s 54ms/step - loss: 1.0302 - acc: 0.6260 - val_loss: 1.0960 - val_acc: 0.6059\n", "Epoch 51/500\n", "10/10 [==============================] - 1s 54ms/step - loss: 0.9768 - acc: 0.6590 - val_loss: 1.0334 - val_acc: 0.6207\n", "Epoch 52/500\n", "10/10 [==============================] - 1s 54ms/step - loss: 0.9697 - acc: 0.6491 - val_loss: 1.0560 - val_acc: 0.6108\n", "Epoch 53/500\n", "10/10 [==============================] - 1s 53ms/step - loss: 0.9674 - acc: 0.6442 - val_loss: 1.1230 - val_acc: 0.5862\n", "Epoch 54/500\n", "10/10 [==============================] - 1s 54ms/step - loss: 0.9524 - acc: 0.6639 - val_loss: 1.1609 - val_acc: 0.6010\n", "Epoch 55/500\n", "10/10 [==============================] - 1s 53ms/step - loss: 0.9607 - acc: 0.6590 - val_loss: 0.9807 - val_acc: 0.6404\n", "Epoch 56/500\n", "10/10 [==============================] - 1s 53ms/step - loss: 0.9383 - acc: 0.6639 - val_loss: 0.9939 - val_acc: 0.6798\n", "Epoch 57/500\n", "10/10 [==============================] - 1s 55ms/step - loss: 0.8993 - acc: 0.6705 - val_loss: 0.9398 - val_acc: 0.6798\n", "Epoch 58/500\n", "10/10 [==============================] - 1s 53ms/step - loss: 0.8422 - acc: 0.7315 - val_loss: 0.9349 - val_acc: 0.6995\n", "Epoch 59/500\n", "10/10 [==============================] - 1s 53ms/step - loss: 0.8382 - acc: 0.7298 - val_loss: 0.8947 - val_acc: 0.7192\n", "Epoch 60/500\n", "10/10 [==============================] - 1s 53ms/step - loss: 0.8002 - acc: 0.7315 - val_loss: 0.8591 - val_acc: 0.7389\n", "Epoch 61/500\n", "10/10 [==============================] - 1s 53ms/step - loss: 0.7990 - acc: 0.7183 - val_loss: 0.9251 - val_acc: 0.7044\n", "Epoch 62/500\n", "10/10 [==============================] - 1s 52ms/step - loss: 0.7855 - acc: 0.7150 - val_loss: 0.8791 - val_acc: 0.7192\n", "Epoch 63/500\n", "10/10 [==============================] - 1s 53ms/step - loss: 0.7614 - acc: 0.7282 - val_loss: 0.8145 - val_acc: 0.7241\n", "Epoch 64/500\n", "10/10 [==============================] - 1s 54ms/step - loss: 0.7542 - acc: 0.7232 - val_loss: 0.7980 - val_acc: 0.7389\n", "Epoch 65/500\n", "10/10 [==============================] - 1s 54ms/step - loss: 0.7588 - acc: 0.7381 - val_loss: 0.7917 - val_acc: 0.7488\n", "Epoch 66/500\n", "10/10 [==============================] - 1s 53ms/step - loss: 0.7414 - acc: 0.7446 - val_loss: 0.7554 - val_acc: 0.7635\n", "Epoch 67/500\n", "10/10 [==============================] - 1s 54ms/step - loss: 0.7006 - acc: 0.7628 - val_loss: 0.7665 - val_acc: 0.7438\n", "Epoch 68/500\n", "10/10 [==============================] - 1s 53ms/step - loss: 0.6955 - acc: 0.7759 - val_loss: 0.7479 - val_acc: 0.7389\n", "Epoch 69/500\n", "10/10 [==============================] - 1s 53ms/step - loss: 0.6535 - acc: 0.7957 - val_loss: 0.7523 - val_acc: 0.7389\n", "Epoch 70/500\n", "10/10 [==============================] - 1s 53ms/step - loss: 0.6829 - acc: 0.7496 - val_loss: 0.7579 - val_acc: 0.7044\n", "Epoch 71/500\n", "10/10 [==============================] - 1s 54ms/step - loss: 0.7376 - acc: 0.7397 - val_loss: 0.8530 - val_acc: 0.7192\n", "Epoch 72/500\n", "10/10 [==============================] - 1s 53ms/step - loss: 0.6708 - acc: 0.7545 - val_loss: 0.7099 - val_acc: 0.7586\n", "Epoch 73/500\n", "10/10 [==============================] - 1s 53ms/step - loss: 0.6318 - acc: 0.7809 - val_loss: 0.7606 - val_acc: 0.7192\n", "Epoch 74/500\n", "10/10 [==============================] - 1s 53ms/step - loss: 0.6451 - acc: 0.7792 - val_loss: 0.6765 - val_acc: 0.7783\n", "Epoch 75/500\n", "10/10 [==============================] - 1s 53ms/step - loss: 0.6597 - acc: 0.7628 - val_loss: 0.7468 - val_acc: 0.7635\n", "Epoch 76/500\n", "10/10 [==============================] - 1s 53ms/step - loss: 0.6078 - acc: 0.7776 - val_loss: 0.8017 - val_acc: 0.6995\n", "Epoch 77/500\n", "10/10 [==============================] - 1s 54ms/step - loss: 0.6989 - acc: 0.7298 - val_loss: 0.8700 - val_acc: 0.6601\n", "Epoch 78/500\n", "10/10 [==============================] - 1s 53ms/step - loss: 0.7624 - acc: 0.7002 - val_loss: 0.7503 - val_acc: 0.7537\n", "Epoch 79/500\n", "10/10 [==============================] - 1s 53ms/step - loss: 0.6992 - acc: 0.7545 - val_loss: 0.7057 - val_acc: 0.7438\n", "Epoch 80/500\n", "10/10 [==============================] - 1s 53ms/step - loss: 0.6426 - acc: 0.7710 - val_loss: 0.6695 - val_acc: 0.7438\n", "Epoch 81/500\n", "10/10 [==============================] - 1s 53ms/step - loss: 0.6381 - acc: 0.7661 - val_loss: 0.8505 - val_acc: 0.7192\n", "Epoch 82/500\n", "10/10 [==============================] - 1s 53ms/step - loss: 0.6625 - acc: 0.7710 - val_loss: 0.6320 - val_acc: 0.7685\n", "Epoch 83/500\n", "10/10 [==============================] - 1s 53ms/step - loss: 0.5765 - acc: 0.8138 - val_loss: 0.7284 - val_acc: 0.7488\n", "Epoch 84/500\n", "10/10 [==============================] - 1s 53ms/step - loss: 0.5680 - acc: 0.7891 - val_loss: 0.6270 - val_acc: 0.7833\n", "Epoch 85/500\n", "10/10 [==============================] - 1s 52ms/step - loss: 0.5501 - acc: 0.8204 - val_loss: 0.6392 - val_acc: 0.7734\n", "Epoch 86/500\n", "10/10 [==============================] - 1s 52ms/step - loss: 0.5593 - acc: 0.8040 - val_loss: 0.6434 - val_acc: 0.8079\n", "Epoch 87/500\n", "10/10 [==============================] - 1s 52ms/step - loss: 0.5556 - acc: 0.8221 - val_loss: 0.6242 - val_acc: 0.7586\n", "Epoch 88/500\n", "10/10 [==============================] - 1s 52ms/step - loss: 0.5580 - acc: 0.7924 - val_loss: 0.6650 - val_acc: 0.7537\n", "Epoch 89/500\n", "10/10 [==============================] - 1s 52ms/step - loss: 0.5440 - acc: 0.7974 - val_loss: 0.6371 - val_acc: 0.7488\n", "Epoch 90/500\n", "10/10 [==============================] - 1s 53ms/step - loss: 0.5241 - acc: 0.8287 - val_loss: 0.5959 - val_acc: 0.7783\n", "Epoch 91/500\n", "10/10 [==============================] - 1s 53ms/step - loss: 0.5337 - acc: 0.8040 - val_loss: 0.5959 - val_acc: 0.7734\n", "Epoch 92/500\n", "10/10 [==============================] - 1s 53ms/step - loss: 0.5345 - acc: 0.8040 - val_loss: 0.6319 - val_acc: 0.8030\n", "Epoch 93/500\n", "10/10 [==============================] - 1s 54ms/step - loss: 0.5505 - acc: 0.7908 - val_loss: 0.7099 - val_acc: 0.7833\n", "Epoch 94/500\n", "10/10 [==============================] - 1s 52ms/step - loss: 0.4974 - acc: 0.8188 - val_loss: 0.6054 - val_acc: 0.7980\n", "Epoch 95/500\n", "10/10 [==============================] - 1s 53ms/step - loss: 0.5167 - acc: 0.8122 - val_loss: 0.5710 - val_acc: 0.7783\n", "Epoch 96/500\n", "10/10 [==============================] - 1s 53ms/step - loss: 0.5239 - acc: 0.8138 - val_loss: 0.5555 - val_acc: 0.8128\n", "Epoch 97/500\n", "10/10 [==============================] - 1s 53ms/step - loss: 0.4725 - acc: 0.8254 - val_loss: 0.5794 - val_acc: 0.7734\n", "Epoch 98/500\n", "10/10 [==============================] - 1s 53ms/step - loss: 0.4453 - acc: 0.8418 - val_loss: 0.5512 - val_acc: 0.8325\n", "Epoch 99/500\n", "10/10 [==============================] - 1s 53ms/step - loss: 0.4455 - acc: 0.8451 - val_loss: 0.5613 - val_acc: 0.7882\n", "Epoch 100/500\n", "10/10 [==============================] - 1s 51ms/step - loss: 0.5076 - acc: 0.7990 - val_loss: 0.6115 - val_acc: 0.7586\n", "Epoch 101/500\n", "10/10 [==============================] - 1s 52ms/step - loss: 0.5241 - acc: 0.7825 - val_loss: 0.6506 - val_acc: 0.7783\n", "Epoch 102/500\n", "10/10 [==============================] - 1s 53ms/step - loss: 0.5290 - acc: 0.8040 - val_loss: 0.5970 - val_acc: 0.7833\n", "Epoch 103/500\n", "10/10 [==============================] - 1s 53ms/step - loss: 0.4884 - acc: 0.8418 - val_loss: 0.5516 - val_acc: 0.7685\n", "Epoch 104/500\n", "10/10 [==============================] - 1s 53ms/step - loss: 0.4876 - acc: 0.8254 - val_loss: 0.5431 - val_acc: 0.8030\n", "Epoch 105/500\n", "10/10 [==============================] - 1s 52ms/step - loss: 0.4490 - acc: 0.8501 - val_loss: 0.5202 - val_acc: 0.7980\n", "Epoch 106/500\n", "10/10 [==============================] - 1s 53ms/step - loss: 0.4435 - acc: 0.8336 - val_loss: 0.5735 - val_acc: 0.7734\n", "Epoch 107/500\n", "10/10 [==============================] - 1s 53ms/step - loss: 0.4739 - acc: 0.8171 - val_loss: 0.6426 - val_acc: 0.7586\n", "Epoch 108/500\n", "10/10 [==============================] - 1s 53ms/step - loss: 0.4792 - acc: 0.8303 - val_loss: 0.5790 - val_acc: 0.7882\n", "Epoch 109/500\n", "10/10 [==============================] - 1s 53ms/step - loss: 0.4347 - acc: 0.8517 - val_loss: 0.4923 - val_acc: 0.8177\n", "Epoch 110/500\n", "10/10 [==============================] - 1s 53ms/step - loss: 0.4183 - acc: 0.8484 - val_loss: 0.5477 - val_acc: 0.7734\n", "Epoch 111/500\n", "10/10 [==============================] - 1s 53ms/step - loss: 0.4031 - acc: 0.8649 - val_loss: 0.4783 - val_acc: 0.8177\n", "Epoch 112/500\n", "10/10 [==============================] - 1s 53ms/step - loss: 0.4151 - acc: 0.8633 - val_loss: 0.5530 - val_acc: 0.8030\n", "Epoch 113/500\n", "10/10 [==============================] - 1s 53ms/step - loss: 0.5256 - acc: 0.7908 - val_loss: 0.5180 - val_acc: 0.7980\n", "Epoch 114/500\n", "10/10 [==============================] - 1s 53ms/step - loss: 0.4435 - acc: 0.8369 - val_loss: 0.4768 - val_acc: 0.8227\n", "Epoch 115/500\n", "10/10 [==============================] - 1s 53ms/step - loss: 0.4344 - acc: 0.8517 - val_loss: 0.4677 - val_acc: 0.8522\n", "Epoch 116/500\n", "10/10 [==============================] - 1s 53ms/step - loss: 0.4191 - acc: 0.8501 - val_loss: 0.5430 - val_acc: 0.8177\n", "Epoch 117/500\n", "10/10 [==============================] - 1s 51ms/step - loss: 0.4064 - acc: 0.8616 - val_loss: 0.5382 - val_acc: 0.8030\n", "Epoch 118/500\n", "10/10 [==============================] - 0s 50ms/step - loss: 0.4156 - acc: 0.8517 - val_loss: 0.5073 - val_acc: 0.7833\n", "Epoch 119/500\n", "10/10 [==============================] - 0s 50ms/step - loss: 0.3816 - acc: 0.8616 - val_loss: 0.4866 - val_acc: 0.8424\n", "Epoch 120/500\n", "10/10 [==============================] - 1s 52ms/step - loss: 0.3925 - acc: 0.8616 - val_loss: 0.4799 - val_acc: 0.8227\n", "Epoch 121/500\n", "10/10 [==============================] - 1s 52ms/step - loss: 0.4507 - acc: 0.8122 - val_loss: 0.5057 - val_acc: 0.8079\n", "Epoch 122/500\n", "10/10 [==============================] - 1s 52ms/step - loss: 0.3905 - acc: 0.8484 - val_loss: 0.4584 - val_acc: 0.8276\n", "Epoch 123/500\n", "10/10 [==============================] - 1s 52ms/step - loss: 0.3958 - acc: 0.8583 - val_loss: 0.5914 - val_acc: 0.7882\n", "Epoch 124/500\n", "10/10 [==============================] - 1s 53ms/step - loss: 0.4351 - acc: 0.8418 - val_loss: 0.4604 - val_acc: 0.8424\n", "Epoch 125/500\n", "10/10 [==============================] - 1s 53ms/step - loss: 0.4016 - acc: 0.8633 - val_loss: 0.5107 - val_acc: 0.8522\n", "Epoch 126/500\n", "10/10 [==============================] - 1s 53ms/step - loss: 0.3805 - acc: 0.8715 - val_loss: 0.4832 - val_acc: 0.8522\n", "Epoch 127/500\n", "10/10 [==============================] - 1s 51ms/step - loss: 0.3413 - acc: 0.8797 - val_loss: 0.4912 - val_acc: 0.8374\n", "Epoch 128/500\n", "10/10 [==============================] - 1s 52ms/step - loss: 0.3511 - acc: 0.8731 - val_loss: 0.4262 - val_acc: 0.8374\n", "Epoch 129/500\n", "10/10 [==============================] - 1s 52ms/step - loss: 0.3300 - acc: 0.8797 - val_loss: 0.4576 - val_acc: 0.8374\n", "Epoch 130/500\n", "10/10 [==============================] - 1s 53ms/step - loss: 0.3470 - acc: 0.8830 - val_loss: 0.4540 - val_acc: 0.8325\n", "Epoch 131/500\n", "10/10 [==============================] - 1s 53ms/step - loss: 0.3503 - acc: 0.8731 - val_loss: 0.4530 - val_acc: 0.8522\n", "Epoch 132/500\n", "10/10 [==============================] - 1s 52ms/step - loss: 0.3702 - acc: 0.8583 - val_loss: 0.4491 - val_acc: 0.8374\n", "Epoch 133/500\n", "10/10 [==============================] - 1s 52ms/step - loss: 0.3601 - acc: 0.8715 - val_loss: 0.4154 - val_acc: 0.8621\n", "Epoch 134/500\n", "10/10 [==============================] - 1s 52ms/step - loss: 0.3075 - acc: 0.9077 - val_loss: 0.3947 - val_acc: 0.8473\n", "Epoch 135/500\n", "10/10 [==============================] - 1s 53ms/step - loss: 0.3160 - acc: 0.8995 - val_loss: 0.4029 - val_acc: 0.8522\n", "Epoch 136/500\n", "10/10 [==============================] - 1s 53ms/step - loss: 0.3409 - acc: 0.8764 - val_loss: 0.4958 - val_acc: 0.8276\n", "Epoch 137/500\n", "10/10 [==============================] - 1s 53ms/step - loss: 0.3857 - acc: 0.8517 - val_loss: 0.5640 - val_acc: 0.8079\n", "Epoch 138/500\n", "10/10 [==============================] - 1s 53ms/step - loss: 0.3959 - acc: 0.8468 - val_loss: 0.5946 - val_acc: 0.8128\n", "Epoch 139/500\n", "10/10 [==============================] - 1s 53ms/step - loss: 0.4518 - acc: 0.8353 - val_loss: 0.6555 - val_acc: 0.7980\n", "Epoch 140/500\n", "10/10 [==============================] - 1s 53ms/step - loss: 0.4182 - acc: 0.8600 - val_loss: 0.4299 - val_acc: 0.8522\n", "Epoch 141/500\n", "10/10 [==============================] - 1s 53ms/step - loss: 0.3432 - acc: 0.8797 - val_loss: 0.4070 - val_acc: 0.8571\n", "Epoch 142/500\n", "10/10 [==============================] - 1s 52ms/step - loss: 0.3021 - acc: 0.9044 - val_loss: 0.4423 - val_acc: 0.8473\n", "Epoch 143/500\n", "10/10 [==============================] - 1s 53ms/step - loss: 0.3165 - acc: 0.8962 - val_loss: 0.4258 - val_acc: 0.8473\n", "Epoch 144/500\n", "10/10 [==============================] - 1s 53ms/step - loss: 0.3073 - acc: 0.8979 - val_loss: 0.4371 - val_acc: 0.8621\n", "Epoch 145/500\n", "10/10 [==============================] - 1s 53ms/step - loss: 0.3393 - acc: 0.8715 - val_loss: 0.3941 - val_acc: 0.8670\n", "Epoch 146/500\n", "10/10 [==============================] - 1s 52ms/step - loss: 0.2985 - acc: 0.8962 - val_loss: 0.3896 - val_acc: 0.8867\n", "Epoch 147/500\n", "10/10 [==============================] - 1s 53ms/step - loss: 0.3306 - acc: 0.8715 - val_loss: 0.4282 - val_acc: 0.8424\n", "Epoch 148/500\n", "10/10 [==============================] - 1s 53ms/step - loss: 0.3446 - acc: 0.8600 - val_loss: 0.4084 - val_acc: 0.8571\n", "Epoch 149/500\n", "10/10 [==============================] - 1s 53ms/step - loss: 0.3140 - acc: 0.8896 - val_loss: 0.4085 - val_acc: 0.8325\n", "Epoch 150/500\n", "10/10 [==============================] - 1s 53ms/step - loss: 0.3092 - acc: 0.8979 - val_loss: 0.3923 - val_acc: 0.8571\n", "Epoch 151/500\n", "10/10 [==============================] - 1s 52ms/step - loss: 0.3008 - acc: 0.8929 - val_loss: 0.3831 - val_acc: 0.8473\n", "Epoch 152/500\n", "10/10 [==============================] - 1s 53ms/step - loss: 0.2825 - acc: 0.9028 - val_loss: 0.3781 - val_acc: 0.8621\n", "Epoch 153/500\n", "10/10 [==============================] - 1s 53ms/step - loss: 0.2859 - acc: 0.9044 - val_loss: 0.3530 - val_acc: 0.8621\n", "Epoch 154/500\n", "10/10 [==============================] - 1s 53ms/step - loss: 0.2695 - acc: 0.9110 - val_loss: 0.3464 - val_acc: 0.8621\n", "Epoch 155/500\n", "10/10 [==============================] - 1s 54ms/step - loss: 0.2721 - acc: 0.9028 - val_loss: 0.3481 - val_acc: 0.8768\n", "Epoch 156/500\n", "10/10 [==============================] - 1s 53ms/step - loss: 0.2712 - acc: 0.9012 - val_loss: 0.3729 - val_acc: 0.8522\n", "Epoch 157/500\n", "10/10 [==============================] - 1s 54ms/step - loss: 0.2571 - acc: 0.9193 - val_loss: 0.3589 - val_acc: 0.8916\n", "Epoch 158/500\n", "10/10 [==============================] - 1s 53ms/step - loss: 0.2535 - acc: 0.9226 - val_loss: 0.3727 - val_acc: 0.8424\n", "Epoch 159/500\n", "10/10 [==============================] - 1s 53ms/step - loss: 0.2558 - acc: 0.9028 - val_loss: 0.3824 - val_acc: 0.8571\n", "Epoch 160/500\n", "10/10 [==============================] - 1s 54ms/step - loss: 0.2656 - acc: 0.8962 - val_loss: 0.3948 - val_acc: 0.8424\n", "Epoch 161/500\n", "10/10 [==============================] - 1s 53ms/step - loss: 0.2643 - acc: 0.9143 - val_loss: 0.4110 - val_acc: 0.8621\n", "Epoch 162/500\n", "10/10 [==============================] - 1s 53ms/step - loss: 0.2796 - acc: 0.8946 - val_loss: 0.4331 - val_acc: 0.8473\n", "Epoch 163/500\n", "10/10 [==============================] - 1s 52ms/step - loss: 0.2647 - acc: 0.9127 - val_loss: 0.3292 - val_acc: 0.8768\n", "Epoch 164/500\n", "10/10 [==============================] - 1s 53ms/step - loss: 0.2454 - acc: 0.9226 - val_loss: 0.4009 - val_acc: 0.8670\n", "Epoch 165/500\n", "10/10 [==============================] - 1s 53ms/step - loss: 0.2810 - acc: 0.9028 - val_loss: 0.4159 - val_acc: 0.8621\n", "Epoch 166/500\n", "10/10 [==============================] - 1s 53ms/step - loss: 0.2732 - acc: 0.8979 - val_loss: 0.4458 - val_acc: 0.8424\n", "Epoch 167/500\n", "10/10 [==============================] - 1s 53ms/step - loss: 0.2621 - acc: 0.8995 - val_loss: 0.3987 - val_acc: 0.8670\n", "Epoch 168/500\n", "10/10 [==============================] - 1s 53ms/step - loss: 0.2543 - acc: 0.9044 - val_loss: 0.3315 - val_acc: 0.8966\n", "Epoch 169/500\n", "10/10 [==============================] - 1s 54ms/step - loss: 0.2233 - acc: 0.9357 - val_loss: 0.3623 - val_acc: 0.8719\n", "Epoch 170/500\n", "10/10 [==============================] - 1s 54ms/step - loss: 0.2224 - acc: 0.9275 - val_loss: 0.3677 - val_acc: 0.8818\n", "Epoch 171/500\n", "10/10 [==============================] - 1s 54ms/step - loss: 0.2287 - acc: 0.9226 - val_loss: 0.3280 - val_acc: 0.8768\n", "Epoch 172/500\n", "10/10 [==============================] - 1s 53ms/step - loss: 0.2828 - acc: 0.8995 - val_loss: 0.3381 - val_acc: 0.8867\n", "Epoch 173/500\n", "10/10 [==============================] - 1s 54ms/step - loss: 0.3064 - acc: 0.8699 - val_loss: 0.3166 - val_acc: 0.8867\n", "Epoch 174/500\n", "10/10 [==============================] - 1s 53ms/step - loss: 0.2722 - acc: 0.9127 - val_loss: 0.5499 - val_acc: 0.8030\n", "Epoch 175/500\n", "10/10 [==============================] - 1s 53ms/step - loss: 0.3437 - acc: 0.8748 - val_loss: 0.3355 - val_acc: 0.8571\n", "Epoch 176/500\n", "10/10 [==============================] - 1s 53ms/step - loss: 0.2390 - acc: 0.9275 - val_loss: 0.3142 - val_acc: 0.8818\n", "Epoch 177/500\n", "10/10 [==============================] - 1s 53ms/step - loss: 0.2427 - acc: 0.9143 - val_loss: 0.3537 - val_acc: 0.8719\n", "Epoch 178/500\n", "10/10 [==============================] - 1s 52ms/step - loss: 0.2441 - acc: 0.9193 - val_loss: 0.3409 - val_acc: 0.8818\n", "Epoch 179/500\n", "10/10 [==============================] - 1s 53ms/step - loss: 0.2421 - acc: 0.9193 - val_loss: 0.3577 - val_acc: 0.8768\n", "Epoch 180/500\n", "10/10 [==============================] - 1s 53ms/step - loss: 0.2366 - acc: 0.9193 - val_loss: 0.3654 - val_acc: 0.8571\n", "Epoch 181/500\n", "10/10 [==============================] - 1s 54ms/step - loss: 0.2623 - acc: 0.9028 - val_loss: 0.3447 - val_acc: 0.8473\n", "Epoch 182/500\n", "10/10 [==============================] - 1s 53ms/step - loss: 0.2285 - acc: 0.9226 - val_loss: 0.4090 - val_acc: 0.8227\n", "Epoch 183/500\n", "10/10 [==============================] - 1s 53ms/step - loss: 0.2568 - acc: 0.9044 - val_loss: 0.4452 - val_acc: 0.8030\n", "Epoch 184/500\n", "10/10 [==============================] - 1s 54ms/step - loss: 0.2966 - acc: 0.8830 - val_loss: 0.3365 - val_acc: 0.8670\n", "Epoch 185/500\n", "10/10 [==============================] - 1s 53ms/step - loss: 0.2757 - acc: 0.8830 - val_loss: 0.3655 - val_acc: 0.8571\n", "Epoch 186/500\n", "10/10 [==============================] - 1s 53ms/step - loss: 0.2254 - acc: 0.9160 - val_loss: 0.4538 - val_acc: 0.8177\n", "Epoch 187/500\n", "10/10 [==============================] - 1s 53ms/step - loss: 0.2774 - acc: 0.8863 - val_loss: 0.2844 - val_acc: 0.8916\n", "Epoch 188/500\n", "10/10 [==============================] - 1s 53ms/step - loss: 0.2239 - acc: 0.9193 - val_loss: 0.3098 - val_acc: 0.8768\n", "Epoch 189/500\n", "10/10 [==============================] - 1s 52ms/step - loss: 0.2231 - acc: 0.9160 - val_loss: 0.3411 - val_acc: 0.8670\n", "Epoch 190/500\n", "10/10 [==============================] - 1s 53ms/step - loss: 0.2073 - acc: 0.9341 - val_loss: 0.3039 - val_acc: 0.8768\n", "Epoch 191/500\n", "10/10 [==============================] - 1s 53ms/step - loss: 0.2079 - acc: 0.9325 - val_loss: 0.2942 - val_acc: 0.8867\n", "Epoch 192/500\n", "10/10 [==============================] - 1s 53ms/step - loss: 0.1861 - acc: 0.9522 - val_loss: 0.2728 - val_acc: 0.8966\n", "Epoch 193/500\n", "10/10 [==============================] - 1s 52ms/step - loss: 0.1798 - acc: 0.9506 - val_loss: 0.3073 - val_acc: 0.8916\n", "Epoch 194/500\n", "10/10 [==============================] - 1s 52ms/step - loss: 0.1965 - acc: 0.9308 - val_loss: 0.3522 - val_acc: 0.8916\n", "Epoch 195/500\n", " 3/10 [========>.....................] - ETA: 0s - loss: 0.1661 - acc: 0.9479"]}, {"ename": "KeyboardInterrupt", "evalue": "", "output_type": "error", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)", "\u001b[1;32mc:\\Work\\Lectures\\LVs\\Notebook-DB\\src\\Computer-Vision-CNNs\\Computer-Vision-CNNs-Exercises-Solutions.ipynb Cell 25\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/c%3A/Work/Lectures/LVs/Notebook-DB/src/Computer-Vision-CNNs/Computer-Vision-CNNs-Exercises-Solutions.ipynb#X35sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m cnn_gap\u001b[39m.\u001b[39;49mfit(X_train, Y_train, batch_size\u001b[39m=\u001b[39;49m\u001b[39m64\u001b[39;49m, epochs\u001b[39m=\u001b[39;49m\u001b[39m500\u001b[39;49m, validation_data\u001b[39m=\u001b[39;49m(X_test, Y_test))\n", "File \u001b[0;32m~/anaconda3/envs/ml-3.8/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n", "File \u001b[0;32m~/anaconda3/envs/ml-3.8/lib/python3.10/site-packages/keras/engine/training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1556\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1557\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1558\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1561\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   1562\u001b[0m ):\n\u001b[1;32m   1563\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1564\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1565\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1566\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n", "File \u001b[0;32m~/anaconda3/envs/ml-3.8/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n", "File \u001b[0;32m~/anaconda3/envs/ml-3.8/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n", "File \u001b[0;32m~/anaconda3/envs/ml-3.8/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    945\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stateless_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    950\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n", "File \u001b[0;32m~/anaconda3/envs/ml-3.8/lib/python3.10/site-packages/tensorflow/python/eager/function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2493\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m   2494\u001b[0m   (graph_function,\n\u001b[1;32m   2495\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2496\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m   2497\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n", "File \u001b[0;32m~/anaconda3/envs/ml-3.8/lib/python3.10/site-packages/tensorflow/python/eager/function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1858\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1859\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1860\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1861\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1862\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1863\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1864\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1865\u001b[0m     args,\n\u001b[1;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1867\u001b[0m     executing_eagerly)\n\u001b[1;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n", "File \u001b[0;32m~/anaconda3/envs/ml-3.8/lib/python3.10/site-packages/tensorflow/python/eager/function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    498\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    500\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    501\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    502\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    503\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    504\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    505\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    506\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    507\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    508\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    511\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    512\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n", "File \u001b[0;32m~/anaconda3/envs/ml-3.8/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n", "\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}], "source": ["cnn_gap.fit(X_train, Y_train, batch_size=64, epochs=500, validation_data=(X_test, Y_test))"]}], "metadata": {"celltoolbar": "Edit Metadata", "kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.12"}}, "nbformat": 4, "nbformat_minor": 2}