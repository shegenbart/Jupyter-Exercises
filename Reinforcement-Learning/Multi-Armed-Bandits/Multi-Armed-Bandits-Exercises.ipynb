{"cells": [{"cell_type": "markdown", "id": "5226d22e", "metadata": {}, "source": ["# Multi-Armed Bandits Exercises \n", "\n", "These exercises are will help you gain a deeper understanding for the mechanisms and behavior\\\n", "of $\\epsilon$-greedy action selection and sample-average based methods using k-armed bandit testbeds. \n", "\n", "**NOTICE:**\n", "1. You are allowed to work in groups of up to three people but **have to document** your group's\\\n", " members in the top cell of your notebook.\n", "2. **Comment your code**, explain what you do (refer to the slides). It will help you understand the topics\\\n", " and help me understand your thinking progress. Quality of comments will be graded. \n", "3. **Discuss** and analyze your results, **write-down your learnings**. These exercises are no programming\\\n", " exercises it is about learning and getting a touch for these methods. Such questions might be asked in the\\\n", " final exams. \n", " 4. Feel free to **experiment** with these methods. Change parameters think about improvements, write down\\\n", " what you learned. This is not only about collecting points for the final grade, it is about understanding\\\n", "  the methods. "]}, {"cell_type": "markdown", "id": "720f8015", "metadata": {}, "source": ["#### Provided Code - The Bandit class "]}, {"cell_type": "code", "execution_count": 58, "id": "6a9c53c9", "metadata": {}, "outputs": [], "source": ["import numpy as np \n", "import matplotlib.pyplot as plt\n", "\n", "class Bandit:\n", "    \n", "    def __init__(self):\n", "        self.n_actions = 10\n", "        # Action values q*(a) are selected according to a normal distribution N(0,1) \n", "        #\n", "        self._action_values = np.random.randn(self.n_actions)\n", "      \n", "    def action_space(self):\n", "        # For the sake of efficiency, the action space will always be from 0 to n_actions-1.\n", "        #\n", "        return np.arange(self.n_actions)\n", "    \n", "    def take_action(self, action):\n", "        assert action in self.action_space(), \"invalid action selected, use action_space() to get a list of valid actions\"\n", "        \n", "        # Actual rewards are selected according to a normal distribution N(mean(q*(At), 1)\n", "        # Compute reward for this action.\n", "        #\n", "        reward = self._action_values[action] + 1 * np.random.randn(1)\n", "        # Check if the optimal action was selected\n", "        #\n", "        is_optimal_action = np.isclose(self._action_values[action], self._action_values.max())\n", "        return reward, is_optimal_action\n", "    \n", "    def action_values(self):\n", "        # Returns the real action values. \n", "        #\n", "        return self._action_values"]}, {"cell_type": "markdown", "id": "6a1a1758", "metadata": {}, "source": ["### Exercise 1: Estimating Action-Values \n", "\n", "**Summary:**\n", "In this exercise you will use the sample-averaging method to estimate action-values for a k-armed bandit. \n", "\n", "**Provided Code:** Use the Bandit class from the cells above. Have a look at each method to figure out what it does. \n", "\n", "**Your Tasks in this exercise:**\n", "\n", "1. Implement the sample-averaging method. \n", "2. Use your implementation to estimate the action-values of the bandit.\n", "    * Compare your estimates with the real-values, how many iterations do you need?\n", "    * Discuss and document your results and learnings. \n", "\n", "\n"]}, {"cell_type": "code", "execution_count": null, "id": "0dbb9805", "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "id": "39fbd2d3", "metadata": {}, "source": ["### Exercise 2 - $\\epsilon$-greedy Action-Selection\n", "\n", "**Summary:**\n", "In this exercise you will use $\\epsilon$-greedy action-selection with sample-averaging method to maximize the expected rewards in a k-armed bandit testbed.  \n", "\n", "**Provided Code:** Use the ```Bandit``` class from the cells above. Have a look at each method to figure out what it does. \n", "\n", "\n", "**Your Tasks in this exercise:**\n", "\n", "1. Implement the sample-averaging method using $\\epsilon$-greedy action-selection. \n", "2. Use your implementation to select the best actions using $\\epsilon$-greedy selection for 1000 time-steps.\n", "    * Use different values for $\\epsilon$, study the behavior. \n", "    * Plot the average rewards over 200 different bandits for each value of $\\epsilon$.\n", "    * Plot the percentage (over 200 different bandits) how often the optimal action was selected at each time-step for each value of $\\epsilon$.\n", "    * Compare your results with the plot in the slides. \n", "    * Discuss and document your results and learnings.\n"]}, {"cell_type": "code", "execution_count": null, "id": "5335ce5c", "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "id": "7053a301", "metadata": {}, "source": ["### Exercise 3 - Nonstationary Bandit\n", "\n", "\n", "**Summary:**\n", "In this exercise you will implement a nonstationary k-armed bandit testbed and use the sample-averaging\\\n", "method on it. \n", "\n", "**Provided Code:** None\n", "\n", "\n", "**Your Tasks in this exercise:**\n", "\n", "1. Extend the ```Bandit``` class such that the reward distributions are nonstationary. To do so:\n", "    * Initialize all $q_*(a)$ to the same value.\n", "    * On each step, take a step in an independent random walk for each action (you can do this by\\\n", "      adding a normally distributed value with mean 0 and standard devition of 0.01 to all $q_*(a)$\\\n", "      on each step).\n", "2. Run the sample-averaging method with $\\epsilon$-greedy action-selection using the nonstationary bandit. \n", "    * Reproduce the plots from exercise 2 using the nonstationary bandit. \n", "    * Interpret, discuss (and document) your results. "]}, {"cell_type": "code", "execution_count": 66, "id": "5b43bd03", "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "id": "56553f8b", "metadata": {}, "source": ["### Exercise 4 - Average-Sampling with Constant Step-Size \n", "\n", "\n", "**Summary:**\n", "In this exercise you will use constant step-size sample-averaging to maximize the\\\n", "reward in a nonstationary k-armed bandit testbed. \n", "\n", "**Provided Code:** None\n", "\n", "\n", "**Your Tasks in this exercise:**\n", "\n", "1. Implement the sample-averaging method using $\\epsilon$-greedy action-selection with a constant step-size and incremental updates. \n", "2. Use your implementation of the nonstationary bandit and your sample-averaging method with constant-step size\\\n", "   to select the best actions.\n", "   * Use  $\\epsilon = 0.1, \\alpha=0.1$\n", "   * Use more time-steps such as $10000$\n", "   * Plot, discuss (and document) your results."]}, {"cell_type": "code", "execution_count": null, "id": "adaf8e6a", "metadata": {}, "outputs": [], "source": []}], "metadata": {"celltoolbar": "Edit Metadata", "kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.16"}}, "nbformat": 4, "nbformat_minor": 5}