{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Artificial Neural Networks - Exercises \n", "\n", "**NOTICE:**\n", "1. You are allowed to work in groups of up to three people but **have to document** your group's\\\n", " members in the top cell of your notebook.\n", "2. **Comment your code**, explain what you do (refer to the slides). It will help you understand the topics\\\n", " and help me understand your thinking progress. Quality of comments will be graded. \n", "3. **Discuss** and analyze your results, **write-down your learnings**. These exercises are no programming\\\n", " exercises it is about learning and getting a touch for these methods. Such questions might be asked in the\\\n", " final exams. \n", " 4. Feel free to **experiment** with these methods. Change parameters think about improvements, write down\\\n", " what you learned. This is not only about collecting points for the final grade, it is about understanding\\\n", "  the methods. "]}, {"cell_type": "code", "execution_count": 308, "metadata": {}, "outputs": [], "source": ["import numpy as np "]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Exercise 1 - Training via Moore-Penrose Inverse\n", "\n", "\n", "**Summary:** In this exercise you will use the Moore-Penrose inverse to \"train\" a single (linear) neuron. It will help you\\\n", "to form a more solid understanding about what training a neural network really means and which shortcomings this method\\\n", "has in a practical setting. It also acts as your first chance to get in touch with **numpy** which is one of the most useful\\\n", "libraries in any machine-learning setting. \n", "\n", "\n", "**Provided Code:** For this exercise I have written a function ```generate_data()``` (see cell below) that can be used to\\\n", "generate some training data. Notice that ```generate_data()``` accepts a lambda (anonymous) function to generate data. Lambdas\\\n", "can be defined like this: ```generate_data(fun=lambda x: 3*x+1)```\n", "\n", "Some useful library functions in this exercise could be: ```numpy.ones(), numpy.column_stack(), numpy.linalg.pinv()```.\\\n", "Use ```numpy.dot()``` to compute a matrix multiplication. \n", "\n", "\n", "\n", "**Your Tasks in this exercise:**\n", "1. Generate Data for any linear (affine) function and find the parameters for a single neuron using\\\n", "the Moore-Penrose inverse. \n", "2. Experiment with different functions and training-set sizes. \n", "3. Try to compute the inverse of large matrices, at what point do you reach a practical limit?\n", "4. Why do you think this method is rarely used in practise (discuss and write-down your answer)?\n", "\n", "\n", "\n"]}, {"cell_type": "code", "execution_count": 291, "metadata": {}, "outputs": [], "source": ["#@title **Provided Code:** ``generate_data()`` function. \n", "import matplotlib.pyplot as plt \n", "import numpy as np \n", "\n", "def generate_data(fun=lambda x:3*x+1, mu=0, std=1, n=10):\n", "    ''' Generates data for Exercise 1. Fun is a lambda (anonymous function)\n", "    that it used to compute the target values (y) for given inputs (x), i.e.:\n", "    y = fun(x).\n", "    '''\n", "    x = mu + std * np.random.randn(n)\n", "    y = fun(x) + 0.3 * np.random.randn(n) # add some noise\n", "    return x,y"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Exercise 2 - Neural Networks as Universal Approximators\n", "\n", "\n", "**Summary:** In this exercise you will experiment with a neural network to understand what the Universal Approximation Theorem tells us. Remember, UAT tell us that neural networks can basically approximate any function.\n", "\n", "\n", "\n", "\n", "**Provided Code:** For this exercise I have provided you with some python code such that will help you train a neural net. Use the ```approximate_NN()``` function to do so. You can supply custom mathematical function of your choice (the function the net tries to approximate) using lambdas such as in the call:\n", "\n", "```python\n", "approximate_NN(lambda x: 3*(x**2) + 6, -10, 10, epochs=50, hidden_layers=1, neurons_per_layer=32)\n", "```\n", "\n", "This will train the weights and biases to approximate the function $f(x):=3x^2 + 6$ within the interval $[-10;10]$ using 50 epochs for training a single layer of hidden neurons with 32 neurons each.\n", "\n", "\n", "\n", "\n", "**Your Tasks in this exercise:**\n", "\n", "1. Experiment and answer the following questions\n", "    * What is the effect of changing the number of neurons vs. the number of hidden layers?\n", "    * What is the effect of having very simple vs. more and more complex functions with regards the the required neurons/layers?\n", "    * Does more training (more epochs) automatically mean better result?\n", "    * Can you find a function that we can not approximate?\n", "    * What happens outside the interval we used for training (indicated by the red dashed lines in the plots)?\n", "    \n", "\n", "\n"]}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["2.10.1\n"]}, {"name": "stderr", "output_type": "stream", "text": ["Der Befehl \"pip\" ist entweder falsch geschrieben oder\n", "konnte nicht gefunden werden.\n"]}], "source": ["#@title **Provided Code:** ``approximate_NN()`` function, and some private helpers. \n", "\n", "!pip install matplotlib --upgrade\n", "\n", "import tensorflow as tf\n", "import numpy as np\n", "from tensorflow import keras\n", "from tensorflow.keras.optimizers import SGD, Adam\n", "from tensorflow.keras.layers import Dense\n", "from tensorflow.keras import Sequential\n", "from tensorflow.keras.activations import relu\n", "\n", "import math\n", "\n", "import matplotlib.pyplot as plt\n", "print(tf.__version__)\n", "\n", "def target_poly(x):\n", "    output = np.empty(len(x))\n", "    for i in range(len(x)):\n", "        output[i] = (1/4.0) * (x[i]+4) * (x[i] + 1) * (x[i] - 2)\n", "    return output\n", "\n", "def target_poly_3(x):\n", "    output = np.empty(len(x))\n", "    for i in range(len(x)):\n", "        output[i] = 2 * x[i]**3 + x[i]**2 - x[i] \n", "    return output\n", "\n", "# polynomial with 8 roots\n", "def target_poly_7(x):\n", "    output = np.empty(len(x))\n", "    for i in range(len(x)):\n", "        output[i] = x[i]**7 + 4*x[i]**6 - 14*x[i]**5 - 56*x[i]**4 + 49*x[i]**3 + 196*x[i]**2 - 36*x[i] - 144\n", "    return output\n", "\n", "# a very high dimensional polygon\n", "def target_poly_14(x):\n", "    output = np.empty(len(x))\n", "    for i in range(len(x)):\n", "        output[i] = x[i]**14 - 0.97*x[i]**13 - 27.5669*x[i]**12 + 28.596911*x[i]**11 + 291.72658501*x[i]**10 - 322.8791018467*x[i]**9 - 1482.7415839005*x[i]**8 + 1741.757256169*x[i]**7 + 3643.0791581657*x[i]**6 - 4525.2996198175*x[i]**5 - 3566.980623085*x[i]**4 + 4745.7344671655*x[i]**3 + 263.42489522071*x[i]**2 - 614.82116925297+x[i] + 66.216725787218\n", "    return output\n", "\n", "def target_sine(x):\n", "    output = np.empty(len(x))\n", "    for i in range(len(x)):\n", "        output[i] = math.sin(x[i])\n", "    return output\n", "\n", "\n", "def mother_wavelet(x):\n", "    return (np.sin(2*np.pi*x) - np.sin(np.pi * x)) / np.pi * x\n", "\n", "\n", "def plot_target(target_func):\n", "    x = np.linspace(-3,3, 1000) \n", "    plt.plot(x,target_func(x), linestyle='--', color='C1', linewidth=2)\n", "    \n", "def combine_neurons(neurons, weights, bias, target_func):\n", "    x = np.arange(-3, 3, 0.01)\n", "    y = np.zeros((len(neurons[0])))\n", "    plt.figure(figsize=(20,8))\n", "    fig, ax = plt.subplots()\n", "    for index in range(len(neurons)):\n", "        y = y + neurons[index] * weights[index]\n", "    y = y + bias\n", "    y2 = target_func(x)\n", "    plt.plot(x, y, label='ReLUs', linewidth=3)\n", "    plt.plot(x, y2, label='Real Function', linestyle='--', color='C1', linewidth=2)\n", "    \n", "    mse = (np.sum((y - y2)**2)) / len(y)\n", "    \n", "    ax.grid(True, which='both')\n", "    ax.axhline(y=0, color='k', linewidth=0.75)\n", "    ax.axvline(x=0, color='k', linewidth=0.75)\n", "    plt.title(\"MSE = %f\" % mse)\n", "    \n", "\n", "def neuron(w, b, plot=True):\n", "    x = np.arange(-3, 3, 0.01)\n", "    y = keras.activations.relu(x * w + b)\n", "    if plot:\n", "        fig, ax = plt.subplots()\n", "        plt.ylim((-10, 10)) \n", "        ax.grid(True, which='both')\n", "        plt.plot(x, y)\n", "    return y\n", "\n", "\n", "def approximate_NN(target_func, min_value=-10, max_value=10, epochs=50, hidden_layers=1, neurons_per_layer=8):\n", "    # Create a NN with a single hidden layer to learn the polynomial function\n", "    #\n", "    model = keras.Sequential()\n", "    for x in range(hidden_layers):\n", "        model.add(Dense(neurons_per_layer, input_dim=1, activation='relu', use_bias=True)) \n", "    model.add(Dense(1,  use_bias=True)) # 1 Neuron\n", "\n", "    sgd = Adam(lr=0.01) # set lower learning rate\n", "    model.compile(sgd, loss='mean_squared_error')\n", "\n", "    # Create some data\n", "    #\n", "    X = np.random.uniform(min_value, max_value, (50000,1))\n", "    Y = target_func(X)\n", "\n", "    model.fit(X,Y, epochs=epochs, batch_size=512);\n", "\n", "    # Plot neural network\n", "    #\n", "    x_test = np.arange(min_value-np.abs(min_value*0.25), max_value+np.abs(max_value*0.25), 0.01)\n", "    y_test_nn = model.predict(x_test)\n", "    y_real = target_func(x_test)\n", "    \n", "    mse = np.sum((np.ravel(y_real) - np.ravel(y_test_nn))**2)\n", "    \n", "    plt.figure(figsize=(8,5))\n", "    plt.axline((min_value, 0), (min_value,10), color=\"r\", linestyle='dashed', linewidth=0.75)\n", "    plt.axline((max_value, 0), (max_value,10), color=\"r\", linestyle='dashed', linewidth=0.75)\n", "    \n", "    plt.plot(x_test, y_test_nn, label='Approximation using NN')\n", "    plt.plot(x_test, y_real, linestyle ='-.', label='Real Function')\n", "    plt.legend()\n", "    \n", "    return model"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": []}], "metadata": {"celltoolbar": "Edit Metadata", "kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.18"}}, "nbformat": 4, "nbformat_minor": 2}