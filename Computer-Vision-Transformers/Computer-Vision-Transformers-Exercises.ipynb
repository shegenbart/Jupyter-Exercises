{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Transformers Exercises\n", "\n", "**NOTICE:**\n", "1. You are allowed to work in groups of up to three people but **have to document** your group's\\\n", " members in the top cell of your notebook.\n", " \n", "2. **Comment your code**, explain what you do (refer to the slides). It will help you understand the topics\\\n", " and help me understand your thinking progress. Quality of comments will be part of the grades. Comments\\\n", " however do not replace answers to explicit questions.\n", "\n", "3. **Discuss** and analyze your results, **write-down your learnings**. These exercises are no programming\\\n", " exercises it is about learning and getting a touch for these methods. The quality and depth of this\\\n", " will be graded. Such questions might be asked in the final exams.\n", "\n", " 4. Feel free to **experiment** with the methods. Change parameters think about improvements, write down\\\n", " what you learned. This is not only about collecting points for the final grade, it is about understanding\\\n", "  the methods. "]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Exercise 1 - Positional Encoding: Implementation\n", "\n", "**Summary:** In this exercise your task is to implement a method that computes the positional encodings\\\n", "as described in the paper *Attention is All you Need*.\n", "\n", "\n", "**Hints:** None\n", "\n", "\n", "**Provided Code:** \n", "See method stub in the cell below. \n", "\n", "\n", "**Your Tasks in this exercise:**\n", "1. Implement the positional encoding as discussed during the lecture using the method stub below. \n", "2. Plot the positional encoding for different positions. \n", "3. Answer and discuss:\n", "    * Why are sinusoidal curves used for the encoding?\n", "    * Why is there a sine and a cosine instead of only a sine used?\n", "    * How does this 10000 term come from? What is the rationale, what are good/bad values?"]}, {"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [], "source": ["import numpy as np \n", "\n", "def pos_encoding(n_tokens, d):\n", "    ''' Returns a matrix of positional encodings for n_tokens with dimension d.\n", "        The i-th matrix row is the positional encoding vector for a token at position\n", "        i.\n", "    '''\n", "    pass\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Exercise 2 - Positional Encoding: Derivation Translation as Linear Transformation\n", "\n", "**Summary:** The positional encodings for $i\\in \\{0,\\dots, \\frac{d}{2}-1\\}$ are given as:\\\n", "$PE(p, 2i) = sin(p \\omega_i)$ and\n", "$PE(p, 2i+1) = cos(p \\omega_i)$,\n", "where $\\omega_i = \\frac{1}{10000^{\\frac{2i}{d}}}$ and $p$ denotes the poken position. \n", "\n", "In this exercise your task is to find a Matrix $M$ such that:\\\n", "$M \\begin{pmatrix}\n", "\t\t\\sin p\\, \\omega_i\\\\\n", "\t\t\\cos p\\, \\omega_i\n", "\t\\end{pmatrix} \t = \n", "\t\\begin{pmatrix}\n", "\t\t\\sin ((p+k) \\omega_i)\\\\\n", "\t\t\\cos ((p+k) \\omega_i)\n", "\t\\end{pmatrix} $.\n", "\n", "\n", "**Hints:** Use the trigonometric addition theorem. Be careful the authors of the paper use sine as the even and cosine\\\n", "as the odd dimensions. \n", "\n", "\n", "**Provided Code:** \n", "None\n", "\n", "\n", "**Your Tasks in this exercise:**\n", "1. Derive the matrix $M$ as described above\n", "2. Answer and discuss:\n", "\t* What is the meaning, that there is such a matrix? \n", "\t* Why is it useful?\n", "    * Why would it be more natural to start with cosine as index 0 instead of sine?\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Exercise 3 - Positional Encoding: Implementation Translation as Linear Transformation\n", "\n", "\n", "\n", "**Summary:** In this exercise your task is to implement a ```def translate(src_token_pos_encoding, position_shift, d)```\\\n", "function. To verify your derivation in the previous exercise. Make sure to support any meaningful value for $d$. \n", "\n", "\n", "**Hints:** None\n", "\n", "\n", "**Provided Code:** \n", "The cell below gives you an idea of how the translate function should be used. \n", "\n", "\n", "**Your Tasks in this exercise:**\n", "1. Implement the translate function. \n", "2. Write code to verify the correctness of your implementation. "]}, {"cell_type": "code", "execution_count": 45, "metadata": {}, "outputs": [], "source": ["d = 50 # Dimensionality of the embedding\n", "n_tokens = 100 # Number of positions\n", "PE = pos_encoding(n_tokens, d) # Create the positional encodings for 100 positions and dim = 50\n", "\n", "\n", "def translate(src_token_pos_encoding, position_shift, d):\n", "    # Implement this\n", "    pass \n", "\n", "# This is an example of how the function is used to shift the positional encoding\n", "# of the token at position 10 by 20 to the right. \n", "#\n", "translated_PE = translate(PE[10,:], 20, d)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Exercise 4 - Projection: Intuition\n", "\n", "\n", "**Summary:** In this exercise you will develop an intuition of the most important operation in attention.\\\n", "Given are two real-valued matrices of token embeddings (each row is a vector representing a token):\n", "\n", "$Q = \n", "\t\t\\begin{pmatrix}\n", "\t\t\tq_{1,1} & \tq_{1,2} & \tq_{1,3} \\\\\n", "\t\t\tq_{2,1} & \tq_{2,2} & \tq_{2,3} \\\\\n", "\t\t\tq_{3,1} & \tq_{3,2} & \tq_{3,3} \\\\\n", "\t\t\\end{pmatrix},\\;\n", "\t\tK =  \\begin{pmatrix}\n", "\t\t\tk_{1,1} & \tk_{1,2} & \tk_{1,3} \\\\\n", "\t\t\tk_{2,1} & \tk_{2,2} & \tk_{2,3} \\\\\n", "\t\t\tk_{3,1} & \tk_{3,2} & \tk_{3,3} \\\\\n", "\t\t\\end{pmatrix}\n", "    $\n", "\n", "\n", "**Your Tasks in this exercise:**\n", "1. Calculate $QK^T$\n", "2. Answer:\n", "\t* What is the intepretation of $QK^T$ ?\n", "\t* How does this relate to cross-correlation?\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["### Exercise 5 - Projection: Self Attention \n", "\n", "\n", "**Summary:** In this exercise you will explore the behavior of self-attention. To do so,\\\n", "I provided you with some code that creates a random square matrix with normalized row-vectors. \n", "\n", "**Your Tasks in this exercise:**\n", "1. Compute the dot product of A with itself ($A A^T$). Try to identify a pattern in the result.  \n", "2. Answer:\n", "\t* Explain the pattern your identified. \n", "\t* What is the interpretation of $A A^T$ \n", "\t* Where is this used in transformers?"]}, {"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [], "source": ["import numpy as np \n", "\n", "# Create a matrix with 10 rows, 5 columns of random values. \n", "#\n", "A = np.random.randn(10,5)\n", "# Normalize each token (row) to be a vector of length 1. \n", "#\n", "A_norm = A / np.linalg.norm(A, axis=1, ord=2)[:, np.newaxis]"]}, {"cell_type": "markdown", "metadata": {}, "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["### Exercise 6 - Projection: Maximum values in Self-Attention\n", "\n", "\n", "**Summary:** In this exercise you will learn another reason why projection and masking is useful. \n", "\n", "\n", "**Your Tasks in this exercise:**\n", "1. Proof mathematically that for two vectors $a,b \\in \\mathbb{R}^n$ with $\\|a\\| = 1,  \\|b\\| = 1$ that $\\langle a, a\\rangle \\geq \\langle a, b\\rangle$.   \n", "**Hint**: Use the Cauchy-Schwarz inequality $|\\langle a, b\\rangle| \\leq \\|a\\| \\|b\\|$.\n", "\n", "2. Explain in words what this means.\n", "3. Explain why this proof is relevant for transformers. \n", "\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Exercise 7 - Attention: Scaled Dot Product Attention\n", "\n", "\n", "\n", "**Summary:** In this exercise your task is to implement the scaled dot product attention. \n", "\n", "**Hints:** None\n", "\n", "\n", "**Provided Code:** \n", "The cell below gives you code used to initialize Q,K,V and shows you how to compute the \n", "attention score using pytorch. \n", "\n", "\n", "**Your Tasks in this exercise:**\n", "1. Implement the ```scaled_dot_product_attention(Q, K, V)``` function.\n", "2. Test your code against the results of ```torch.nn.functional.scaled_dot_product_attention```"]}, {"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [{"ename": "", "evalue": "", "output_type": "error", "traceback": ["\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n", "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n", "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n", "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."]}], "source": ["import torch\n", "import numpy as np  \n", "\n", "embedding_dim = 3\n", "seq = np.random.randn(5, embedding_dim)\n", "Q = torch.tensor(seq, dtype=torch.float32)\n", "K = Q\n", "V = Q \n", "\n", "ref_attention_scores = torch.nn.functional.scaled_dot_product_attention(Q, K, V)\n", "\n", "print(ref_attention_scores)\n", "\n", "\n", "def scaled_dot_product_attention(Q, K, V):\n", "    # Implement this\n", "    pass"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.11.9"}}, "nbformat": 4, "nbformat_minor": 2}