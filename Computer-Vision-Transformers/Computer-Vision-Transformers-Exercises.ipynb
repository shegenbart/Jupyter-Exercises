{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Transformers Exercises\n", "\n", "**NOTICE:**\n", "1. You are allowed to work in groups of up to three people but **have to document** your group's\\\n", " members in the top cell of your notebook.\n", " \n", "2. **Comment your code**, explain what you do (refer to the slides). It will help you understand the topics\\\n", " and help me understand your thinking progress. Quality of comments will be part of the grades. Comments\\\n", " however do not replace answers to explicit questions.\n", "\n", "3. **Discuss** and analyze your results, **write-down your learnings**. These exercises are no programming\\\n", " exercises it is about learning and getting a touch for these methods. The quality and depth of this\\\n", " will be graded. Such questions might be asked in the final exams.\n", "\n", " 4. Feel free to **experiment** with the methods. Change parameters think about improvements, write down\\\n", " what you learned. This is not only about collecting points for the final grade, it is about understanding\\\n", "  the methods. "]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Exercise * - Positional Encoding\n", "\n", "**Summary:** In this exercise your task is to implement a method that computes the positional encodings\\\n", "as described in the paper *Attention is All you Need*.\n", "\n", "\n", "**Hints:** None\n", "\n", "\n", "**Provided Code:** \n", "See method stub in the cell below. \n", "\n", "\n", "**Your Tasks in this exercise:**\n", "1. Implement the positional encoding as discussed during the lecture using the method stub below. \n", "2. Plot the positional encoding for different positions. \n", "3. Answer and discuss:\n", "    * Why are sinusoidal curves used for the encoding?\n", "    * Why is there a sine and a cosine instead of only a sine used?\n", "    * How does this 10000 term come from? What is the rationale, what are good/bad values?"]}, {"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [], "source": ["import numpy as np \n", "\n", "def pos_encoding(n_tokens, d):\n", "    ''' Returns a matrix of positional encodings for n_tokens with dimension d.\n", "        The i-th matrix row is the positional encoding vector for a token at position\n", "        i.\n", "    '''\n", "    pass\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Exercise * - Derivation: Translation in Positional Encoding as Linear Transformation\n", "\n", "**Summary:** The positional encodings for $i\\in \\{0,\\dots, \\frac{d}{2}-1\\}$ are given as:\\\n", "$PE(p, 2i) = sin(p \\omega_i)$ and\n", "$PE(p, 2i+1) = cos(p \\omega_i)$,\n", "where $\\omega_i = \\frac{1}{10000^{\\frac{2i}{d}}}$ and $p$ denotes the poken position. \n", "\n", "In this exercise your task is to find a Matrix $M$ such that:\\\n", "$M \\begin{pmatrix}\n", "\t\t\\sin p\\, \\omega_i\\\\\n", "\t\t\\cos p\\, \\omega_i\n", "\t\\end{pmatrix} \t = \n", "\t\\begin{pmatrix}\n", "\t\t\\sin ((p+k) \\omega_i)\\\\\n", "\t\t\\cos ((p+k) \\omega_i)\n", "\t\\end{pmatrix} $.\n", "\n", "\n", "**Hints:** Use the trigonometric addition theorem. Be careful the authors of the paper use sine as the even and cosine\\\n", "as the odd dimensions. \n", "\n", "\n", "**Provided Code:** \n", "None\n", "\n", "\n", "**Your Tasks in this exercise:**\n", "1. Derive the matrix $M$ as described above\n", "2. Answer and discuss:\n", "\t* What is the meaning, that there is such a matrix? \n", "\t* Why is it useful?\n", "    * Why would it be more natural to start with cosine as index 0 instead of sine?\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Exercise * - Implementation: Translation in Positional Encodings as Linear Transformation\n", "\n", "\n", "**Summary:** In this exercise your task is to implement a ```def translate(src_token_pos_encoding, position_shift, d)```\\\n", "function. To verify your derivation in the previous exercise. Make sure to support any meaningful value for $d$. \n", "\n", "\n", "**Hints:** None\n", "\n", "\n", "**Provided Code:** \n", "The cell below gives you an idea of how the translate function should be used. \n", "\n", "\n", "**Your Tasks in this exercise:**\n", "1. Implement the translate function. \n", "2. Write code to verify the correctness of your implementation. "]}, {"cell_type": "code", "execution_count": 45, "metadata": {}, "outputs": [], "source": ["d = 50 # Dimensionality of the embedding\n", "n_tokens = 100 # Number of positions\n", "PE = pos_encoding(n_tokens, d) # Create the positional encodings for 100 positions and dim = 50\n", "\n", "\n", "def translate(src_token_pos_encoding, position_shift, d):\n", "    # Implement this\n", "    pass \n", "\n", "# This is an example of how the function is used to shift the positional encoding\n", "# of the token at position 10 by 20 to the right. \n", "#\n", "translated_PE = translate(PE[10,:], 20, d)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": []}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.11.9"}}, "nbformat": 4, "nbformat_minor": 2}