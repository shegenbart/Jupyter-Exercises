{"cells": [{"cell_type": "markdown", "id": "9a3eb38a", "metadata": {}, "source": ["# Neural Networks\n", "\n", "This notebook contains the exercises for the neural network slides. "]}, {"cell_type": "markdown", "id": "61bd20c7", "metadata": {}, "source": ["### Helper functions:"]}, {"cell_type": "code", "execution_count": 51, "id": "4b1fecf5", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'c:\\\\anaconda\\\\envs\\\\ml-3.8\\\\lib\\\\site-packages\\\\matplotlib\\\\backends\\\\_backend_agg.cp38-win_amd64.pyd'\n", "Consider using the `--user` option or check the permissions.\n", "\n"]}, {"name": "stdout", "output_type": "stream", "text": ["Requirement already satisfied: matplotlib in c:\\anaconda\\envs\\ml-3.8\\lib\\site-packages (3.5.1)\n", "Collecting matplotlib\n", "  Downloading matplotlib-3.6.3-cp38-cp38-win_amd64.whl (7.2 MB)\n", "     ---------------------------------------- 7.2/7.2 MB 11.8 MB/s eta 0:00:00\n", "Requirement already satisfied: python-dateutil>=2.7 in c:\\anaconda\\envs\\ml-3.8\\lib\\site-packages (from matplotlib) (2.8.2)\n", "Collecting contourpy>=1.0.1\n", "  Downloading contourpy-1.0.7-cp38-cp38-win_amd64.whl (162 kB)\n", "     ---------------------------------------- 163.0/163.0 kB ? eta 0:00:00\n", "Requirement already satisfied: pillow>=6.2.0 in c:\\anaconda\\envs\\ml-3.8\\lib\\site-packages (from matplotlib) (9.0.0)\n", "Requirement already satisfied: packaging>=20.0 in c:\\anaconda\\envs\\ml-3.8\\lib\\site-packages (from matplotlib) (22.0)\n", "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\anaconda\\envs\\ml-3.8\\lib\\site-packages (from matplotlib) (1.4.0)\n", "Requirement already satisfied: cycler>=0.10 in c:\\anaconda\\envs\\ml-3.8\\lib\\site-packages (from matplotlib) (0.11.0)\n", "Requirement already satisfied: pyparsing>=2.2.1 in c:\\anaconda\\envs\\ml-3.8\\lib\\site-packages (from matplotlib) (3.0.9)\n", "Requirement already satisfied: fonttools>=4.22.0 in c:\\anaconda\\envs\\ml-3.8\\lib\\site-packages (from matplotlib) (4.31.1)\n", "Requirement already satisfied: numpy>=1.19 in c:\\anaconda\\envs\\ml-3.8\\lib\\site-packages (from matplotlib) (1.22.4)\n", "Requirement already satisfied: six>=1.5 in c:\\anaconda\\envs\\ml-3.8\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n", "Installing collected packages: contourpy, matplotlib\n", "  Attempting uninstall: matplotlib\n", "    Found existing installation: matplotlib 3.5.1\n", "    Uninstalling matplotlib-3.5.1:\n", "2.10.0\n"]}], "source": ["!pip install matplotlib --upgrade\n", "\n", "import tensorflow as tf\n", "import numpy as np\n", "from tensorflow import keras\n", "from tensorflow.keras.optimizers import SGD, Adam\n", "from tensorflow.keras.layers import Dense\n", "from tensorflow.keras import Sequential\n", "import math\n", "\n", "import matplotlib.pyplot as plt\n", "print(tf.__version__)\n", "\n", "def target_poly(x):\n", "    output = np.empty(len(x))\n", "    for i in range(len(x)):\n", "        output[i] = (1/4.0) * (x[i]+4) * (x[i] + 1) * (x[i] - 2)\n", "    return output\n", "\n", "def target_poly_3(x):\n", "    output = np.empty(len(x))\n", "    for i in range(len(x)):\n", "        output[i] = 2 * x[i]**3 + x[i]**2 - x[i] \n", "    return output\n", "\n", "# polynomial with 8 roots\n", "def target_poly_7(x):\n", "    output = np.empty(len(x))\n", "    for i in range(len(x)):\n", "        output[i] = x[i]**7 + 4*x[i]**6 - 14*x[i]**5 - 56*x[i]**4 + 49*x[i]**3 + 196*x[i]**2 - 36*x[i] - 144\n", "    return output\n", "\n", "# a very high dimensional polygon\n", "def target_poly_14(x):\n", "    output = np.empty(len(x))\n", "    for i in range(len(x)):\n", "        output[i] = x[i]**14 - 0.97*x[i]**13 - 27.5669*x[i]**12 + 28.596911*x[i]**11 + 291.72658501*x[i]**10 - 322.8791018467*x[i]**9 - 1482.7415839005*x[i]**8 + 1741.757256169*x[i]**7 + 3643.0791581657*x[i]**6 - 4525.2996198175*x[i]**5 - 3566.980623085*x[i]**4 + 4745.7344671655*x[i]**3 + 263.42489522071*x[i]**2 - 614.82116925297+x[i] + 66.216725787218\n", "    return output\n", "\n", "def target_sine(x):\n", "    output = np.empty(len(x))\n", "    for i in range(len(x)):\n", "        output[i] = math.sin(x[i])\n", "    return output\n", "\n", "\n", "def mother_wavelet(x):\n", "    return (np.sin(2*np.pi*x) - np.sin(np.pi * x)) / np.pi * x\n", "\n", "\n", "def plot_target(target_func):\n", "    x = np.linspace(-3,3, 1000) \n", "    plt.plot(x,target_func(x), linestyle='--', color='C1', linewidth=2)\n", "    \n", "def combine_neurons(neurons, weights, bias, target_func):\n", "    x = np.arange(-3, 3, 0.01)\n", "    y = np.zeros((len(neurons[0])))\n", "    plt.figure(figsize=(20,8))\n", "    fig, ax = plt.subplots()\n", "    for index in range(len(neurons)):\n", "        y = y + neurons[index] * weights[index]\n", "    y = y + bias\n", "    y2 = target_func(x)\n", "    plt.plot(x, y, label='ReLUs', linewidth=3)\n", "    plt.plot(x, y2, label='Real Function', linestyle='--', color='C1', linewidth=2)\n", "    \n", "    mse = (np.sum((y - y2)**2)) / len(y)\n", "    \n", "    ax.grid(True, which='both')\n", "    ax.axhline(y=0, color='k', linewidth=0.75)\n", "    ax.axvline(x=0, color='k', linewidth=0.75)\n", "    plt.title(\"MSE = %f\" % mse)\n", "    \n", "from tensorflow.keras.activations import relu\n", "def neuron(w, b, plot=True):\n", "    x = np.arange(-3, 3, 0.01)\n", "    y = keras.activations.relu(x * w + b)\n", "    if plot:\n", "        fig, ax = plt.subplots()\n", "        plt.ylim((-10, 10)) \n", "        ax.grid(True, which='both')\n", "        plt.plot(x, y)\n", "    return y\n", "\n", "\n", "def approximate_NN(target_func, min_value=-10, max_value=10, epochs=50, hidden_layers=1, neurons_per_layer=8):\n", "    # Create a NN with a single hidden layer to learn the polynomial function\n", "    #\n", "    model = keras.Sequential()\n", "    for x in range(hidden_layers):\n", "        model.add(Dense(neurons_per_layer, input_dim=1, activation='relu', use_bias=True)) \n", "    model.add(Dense(1,  use_bias=True)) # 1 Neuron\n", "\n", "    sgd = Adam(lr=0.01) # set lower learning rate\n", "    model.compile(sgd, loss='mean_squared_error')\n", "\n", "    # Create some data\n", "    #\n", "    X = np.random.uniform(min_value, max_value, (50000,1))\n", "    Y = target_func(X)\n", "\n", "    model.fit(X,Y, epochs=epochs, batch_size=512);\n", "\n", "    # Plot neural network\n", "    #\n", "    x_test = np.arange(min_value-np.abs(min_value*0.25), max_value+np.abs(max_value*0.25), 0.01)\n", "    y_test_nn = model.predict(x_test)\n", "    y_real = target_func(x_test)\n", "    \n", "    mse = np.sum((np.ravel(y_real) - np.ravel(y_test_nn))**2)\n", "    \n", "    plt.figure(figsize=(8,5))\n", "    plt.axline((min_value, 0), (min_value,10), color=\"r\", linestyle='dashed', linewidth=0.75)\n", "    plt.axline((max_value, 0), (max_value,10), color=\"r\", linestyle='dashed', linewidth=0.75)\n", "    \n", "    plt.plot(x_test, y_test_nn, label='Approximation using NN')\n", "    plt.plot(x_test, y_real, linestyle ='-.', label='Real Function')\n", "    plt.legend()\n", "    \n", "    return model"]}, {"cell_type": "markdown", "id": "01ac1a66", "metadata": {}, "source": ["### Exercise 1:\n", "\n", "Remember how the universal approximation theorems tell us that neural networks can basically approximate any function. \n", "In this exercise we will use the ```approximate_NN()``` to test this theorem. You can supply custom mathematical function to approximate using lambdas such as the call:\n", "\n", "```python\n", "approximate_NN(lambda x: 3*(x**2) + 6, -10, 10, epochs=50, hidden_layers=1, neurons_per_layer=32)\n", "```\n", "\n", "will try to approximate the function $f(x):=3x^2 + 6$ within the interval $[-10;10]$ using 50 epochs for training a single layer of hidden neurons with 32 neurons each.\n", "\n", "I have also supplied you with some more complex functions (```target_poly```, ```target_poly3```,  ```target_poly7```, ```target_poly14```, ```mother_wavelet```).\n", "\n", "Try to answer these questions:\n", "* What is the effect of changing the number of neurons vs. the number of hidden layers?\n", "* What is the effect of having very simple vs. more and more complex functions with regards the the required neurons/layers?\n", "* Does more training (more epochs) automatically mean better result?\n", "* Can you find a function that we can not approximate?\n", "* What happens outside the interval we used for training (indicated by the red dashed lines in the plots)?   \n", "\n", "\n", "\n", "    "]}, {"cell_type": "markdown", "id": "20551992", "metadata": {}, "source": ["### Exercise 2:\n", "\n", "In this exercise you are provided with a very simple neuron using a ReLU as activation function. You can supply the neurons parameters manually (the weight and bias term) as ```neuron(3, 2)``` will compute and plot all the output values generated by the neuron for all input values in an interval of $[-3;3]$. **Hint:** You can use the semi-colon in the call ```neuron(3, 2);``` to suppress the output. \n", "\n", "\n", "**Questions:**\n", "* What is the effect of changing the weight?\n", "* What is the effect of changing the bias?\n", "\n", "\n"]}, {"cell_type": "code", "execution_count": 11, "id": "0989d9ff", "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "id": "b5060133", "metadata": {}, "source": ["### Exercise 3:\n", "\n", "A mathematical function is linear iff $f(x)+f(y) = f(x+y)$ and $\\lambda f(x) = f(\\lambda x)$.\n", "\n", "* Show that ReLU is not a linear function. "]}, {"cell_type": "markdown", "id": "932796f9", "metadata": {}, "source": ["### Exercise 4:\n", "\n", "You saw that we can combine neurons to a neural network. Try to manually find the weights for approximating the ```target_poly_3``` function.  Use the ```combine_neurons``` function to combine multiple neurons. \n", "\n", "```python \n", "def combine_neurons(neurons, weights, bias, target_func):\n", "    \"\"\" Parameters:\n", "    neurons - a list of neurons such sa created by the neuron() function\n", "    weights - a list of weights used by the output layer neuron\n", "    bias - a single bias value used by the output layer neuron\n", "    target_func - a function to plot as reference for approximation\n", "    \"\"\"\n", "    \n", "```\n", "\n", "* Manually find the parameters of a neural network to approximate the ```target_poly_3``` function (*You can do this via trial and error, no calculations are necessary*).\n", "* Try to find the parameters of a neural network to approximate the ```target_sine``` function (*this might be difficult*).\n", "* Use the call to ```model = approximate_NN(...)``` to let tensorflow find the weights and biases for approximating the ```target_sine``` function. Have a look at ```model.layers[0].weights``` and ```model.layers[1].weights``` to help you find appropriate parameters for this problem. \n", "\n", "    "]}, {"cell_type": "code", "execution_count": 178, "id": "b0bf1a56", "metadata": {}, "outputs": [{"data": {"text/plain": ["<Figure size 1440x576 with 0 Axes>"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"image/png": "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", "text/plain": ["<Figure size 432x288 with 1 Axes>"]}, "metadata": {"needs_background": "light"}, "output_type": "display_data"}], "source": ["# Combine two neurons \n", "#\n", "r1 = neuron(3,0, False)\n", "r2 = neuron(-3,0, False)\n", "# r1 is weighted by 1 and -1 with a bias of -5 in the output layer neuron\n", "#\n", "combine_neurons([r1, r2], [1, -1], -5, target_poly_3)"]}, {"cell_type": "markdown", "id": "9c3ea290", "metadata": {}, "source": ["## Exercise 6: Stochastic Gradient Descent\n", "\n", "In gradient descent we move along the negative gradient to find a minimum of a function. This is the way neural networks are trained today. I have supplied you with an implementation of gradient descent \n", "```SGD(func, start_value, xmin = -2.5, xmax = 2, learning_rate=0.1, iterations=10)```. I have also provided an interesting target polynomial ```target``` for you. \n", "\n", "* Try to find the minimum within the interval $[-2.5;2]$ of the target polynomial using ```SGD```.\n", "* What is the impact of the learning rate (too high or too low) on the result?\n", "* What is the impact of the number of iterations on the result?\n", "\n", "Experiment with different start values and see how the algorithm performs depending on the chosen parameters for learning rate and iterations. "]}, {"cell_type": "code", "execution_count": 168, "id": "8f83a34c", "metadata": {}, "outputs": [], "source": ["# A very high dimensional polygon\n", "def target(x):\n", "    output = np.empty(len(x))\n", "    for i in range(len(x)):\n", "        output[i] = 0.001 * (x[i]**14 - 0.97*x[i]**13 - 27.5669*x[i]**12 + 28.596911*x[i]**11 + 291.72658501*x[i]**10 - 322.8791018467*x[i]**9 - 1482.7415839005*x[i]**8 + 1741.757256169*x[i]**7 + 3643.0791581657*x[i]**6 - 4525.2996198175*x[i]**5 - 3566.980623085*x[i]**4 + 4745.7344671655*x[i]**3 + 263.42489522071*x[i]**2 - 614.82116925297+x[i] + 66.216725787218)\n", "    return output\n", "\n", "def SGD(func, start_value, xmin = -2.5, xmax = 2, learning_rate=0.1, iterations=10):\n", "    min_values = []\n", "\n", "    min_values.append(start_value)\n", "    min_value = start_value\n", "    delta = 0.01\n", "    diverge = False\n", "    \n", "    for i in range(iterations):\n", "        # Compute approximative derivative numerically, usually we do this\n", "        # with a correctly compute partial derivative\n", "        f1 = func([min_value + delta])\n", "        f2 = func([min_value - delta])\n", "        derivative = (f1[0] - f2[0]) / (2*delta)\n", " \n", "        # Move the opt_value along the derivative\n", "        min_value = min_value - learning_rate * derivative\n", "         \n", "        # Check if we diverge to save the plot\n", "        if min_value < xmin:\n", "            diverge= True\n", "            min_values.append(xmin)\n", "            break\n", "        if min_value > xmax:\n", "            diverge= True\n", "            min_values.append(xmax)\n", "            break\n", "            \n", "        min_values.append(min_value)\n", "        \n", "    # For plotting only\n", "    fig, ax = plt.subplots(figsize=(12,6))\n", "    x = np.arange(xmin, xmax, 0.01)\n", "    y = func(x)\n", "    plt.plot(x, y)\n", "    \n", "    fm = func(min_values)\n", "    plt.plot(min_values, fm, 'ro-')\n", "    ax.grid(True, which='both')\n", "    \n", "    if diverge:\n", "        plt.title('Optimization Diverged at: %f' % min_value)\n", "    \n", "    return min_value"]}, {"cell_type": "markdown", "id": "514e97ef", "metadata": {}, "source": ["### Exercise 7: Stochastic Gradient Descent with Momentum\n", "\n", "In gradient descent with momentumm we move along the negative gradient to find a minimum of a function but use our previous updates to skip over small bumps.I have supplied you with an implementation of gradient descent with momentum ```SGD_Momentum(func, start_value, xmin = -2.5, xmax = 2, learning_rate=0.1, momentum = 0.1, iterations=10)```. I have also provided an interesting target polynomial ```target``` for you. \n", "\n", "* Try to find the minimum within the interval $[-2.5;2]$ of the target polynomial using ```SGD with momentum```.\n", "* What is the impact of the learning rate (too high or too low) on the result?\n", "* What is the impact of the momentum (too high or too low) on the result?\n", "* What is the impact of the number of iterations on the result?\n", "\n", "Experiment with different start values and see how the algorithm performs depending on the chosen parameters for learning rate and iterations. "]}, {"cell_type": "code", "execution_count": 169, "id": "c48b8420", "metadata": {}, "outputs": [], "source": ["import math\n", "\n", "\n", "def SGD_Momentum(func, start_value, xmin = -2.5, xmax = 2, learning_rate=0.1, momentum = 0.1, iterations=10):\n", "    min_values = []\n", "\n", "    min_values.append(start_value)\n", "    min_value = start_value\n", "    \n", "    update_min_value = 0\n", "    delta = 0.01\n", "    diverge = False\n", "    \n", "    for i in range(iterations):\n", "        # Compute approximative derivative numerically, usually we do this\n", "        # with symbolically computed partial derivatives\n", "        f1 = func([min_value + delta])\n", "        f2 = func([min_value - delta])\n", "        derivative = (f1[0] - f2[0]) / (2*delta)\n", " \n", "        # Move the opt_value along the derivative with Momentum\n", "        update_min_value = momentum * update_min_value - learning_rate * derivative\n", "        min_value = min_value + update_min_value\n", "        \n", "        # Check if we diverge to save the plot\n", "        if min_value < xmin:\n", "            diverge= True\n", "            min_values.append(xmin)\n", "            break\n", "        if min_value > xmax:\n", "            diverge= True\n", "            min_values.append(xmax)\n", "            break\n", "            \n", "        min_values.append(min_value)   \n", "        \n", "        \n", "    # For plotting only\n", "    fig, ax = plt.subplots(figsize=(12,6))\n", "    x = np.arange(xmin, xmax, 0.01)\n", "    y = func(x)\n", "    plt.plot(x, y)\n", "    \n", "    fm = func(min_values)\n", "    plt.plot(min_values, fm, 'ro-')\n", "    plt.plot(min_values[-1], fm[-1], 'go')\n", "    plt.plot(min_values[0], fm[0], 'bo')\n", "    ax.grid(True, which='both')\n", "    \n", "    if diverge:\n", "        plt.title('Optimization Diverged at: %f' % min_value)\n", "    return min_value"]}, {"cell_type": "markdown", "id": "09a2c06e", "metadata": {}, "source": ["## Exercise 8\n", "\n", "In this exercise your job is to create a neural network that allows us to classify the baseball dataset. The dataset is comprised of a bunch of statistics per player. Your job is to create a neural-network that is capable of predicting wether the player will be accepted into the hall of fame or not. \n", "\n", "* Load the baseball dataset using ```load_dataset(../data/baseball.pickle)```.\n", "* Have a look at the dataset, there are multiple fields such as Attributes, Targets, Description, ...\n", "* Create a neural network using ```keras``` to predict if a player will be accepted into the the hall-of-fame or not.\n", "* Evaluate your model using appropriate cross-validation schemes. "]}, {"cell_type": "code", "execution_count": 210, "id": "c1a2d106", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["'wget' is not recognized as an internal or external command,\n", "operable program or batch file.\n"]}], "source": ["# Download data\n", "#\n", "!wget https://github.com/shegenbart/Jupyter-Exercises/raw/main/data/baseball.pickle -P ../data\n", "\n", "#Imports, Helper Functions and Data Structures\n", "#\n", "import numpy as np\n", "import pickle\n", "\n", "from dataclasses import dataclass\n", "def load_dataset(filename):\n", "    with open(filename, 'rb') as fd:\n", "        dataset = pickle.load(fd)\n", "    return dataset\n", "\n", "# Baseball dataset\n", "#\n", "@dataclass\n", "class Dataset:\n", "    Description: str\n", "    Attributes: list()\n", "    Targets: list()\n", "    X: np.array\n", "    Y: np.array"]}], "metadata": {"celltoolbar": "Edit Metadata", "kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.12"}, "vscode": {"interpreter": {"hash": "71e3996440a4286f4b5430a3d4d1ae66fa68d894e5edac3334c7ebe5f98b7546"}}}, "nbformat": 4, "nbformat_minor": 5}