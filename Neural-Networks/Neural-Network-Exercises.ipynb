{"cells": [{"cell_type": "markdown", "id": "c9004193", "metadata": {}, "source": ["# Neural Networks\n", "\n", "This notebook contains the exercises for the neural network slides. "]}, {"cell_type": "markdown", "id": "789e312b", "metadata": {}, "source": ["### Helper functions:"]}, {"cell_type": "code", "execution_count": 126, "id": "58f7a120", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["2.3.0\n"]}], "source": ["import tensorflow as tf\n", "import numpy as np\n", "from tensorflow import keras\n", "from tensorflow.keras.optimizers import SGD\n", "from tensorflow.keras.optimizers import Adam\n", "\n", "import matplotlib.pyplot as plt\n", "print(tf.__version__)\n", "\n", "def target_poly(x):\n", "    output = np.empty(len(x))\n", "    for i in range(len(x)):\n", "        output[i] = (1/4.0) * (x[i]+4) * (x[i] + 1) * (x[i] - 2)\n", "    return output\n", "\n", "# polynomial with 8 roots\n", "def target_poly_7(x):\n", "    output = np.empty(len(x))\n", "    for i in range(len(x)):\n", "        output[i] = x[i]**7 + 4*x[i]**6 - 14*x[i]**5 - 56*x[i]**4 + 49*x[i]**3 + 196*x[i]**2 - 36*x[i] - 144\n", "    return output\n", "\n", "# a very high dimensional polygon\n", "def target_poly_14(x):\n", "    output = np.empty(len(x))\n", "    for i in range(len(x)):\n", "        output[i] = x[i]**14 - 0.97*x[i]**13 - 27.5669*x[i]**12 + 28.596911*x[i]**11 + 291.72658501*x[i]**10 - 322.8791018467*x[i]**9 - 1482.7415839005*x[i]**8 + 1741.757256169*x[i]**7 + 3643.0791581657*x[i]**6 - 4525.2996198175*x[i]**5 - 3566.980623085*x[i]**4 + 4745.7344671655*x[i]**3 + 263.42489522071*x[i]**2 - 614.82116925297+x[i] + 66.216725787218\n", "    return output\n", "\n", "def mother_wavelet(x):\n", "    return (np.sin(2*np.pi*x) - np.sin(np.pi * x)) / np.pi * x\n", "\n", "def approximate_NN(target_func, min_value=-10, max_value=10, epochs=50, hidden_layers=1, neurons_per_layer=8):\n", "    # Create a NN with a single hidden layer to learn the polynomial function\n", "    #\n", "    model = keras.Sequential()\n", "    for x in range(hidden_layers):\n", "        model.add(Dense(neurons_per_layer, input_dim=1, activation='relu', use_bias=True)) \n", "    model.add(Dense(1,  use_bias=True)) # 1 Neuron\n", "\n", "    sgd = Adam(lr=0.01) # set lower learning rate\n", "    model.compile(sgd, loss='mean_squared_error')\n", "\n", "    # Create some data\n", "    #\n", "    X = np.random.uniform(min_value, max_value, (50000,1))\n", "    Y = target_func(X)\n", "\n", "    model.fit(X,Y, epochs=epochs, batch_size=512);\n", "\n", "    # Plot neural network\n", "    #\n", "    x_test = np.arange(min_value-np.abs(min_value*0.25), max_value+np.abs(max_value*0.25), 0.01)\n", "    y_test_nn = model.predict(x_test)\n", "    y_real = target_func(x_test)\n", "    \n", "    mse = np.sum((np.ravel(y_real) - np.ravel(y_test_nn))**2)\n", "    \n", "    plt.axline((min_value, 0), (min_value,10), color=\"r\", linestyle='dashed', linewidth=0.75)\n", "    plt.axline((max_value, 0), (max_value,10), color=\"r\", linestyle='dashed', linewidth=0.75)\n", "    \n", "    plt.plot(x_test, y_test_nn, label='Approximation using NN')\n", "    plt.plot(x_test, y_real, linestyle ='-.', label='Real Function')\n", "    plt.legend()"]}, {"cell_type": "markdown", "id": "c799ff0c", "metadata": {}, "source": ["### Exercise 1:\n", "\n", "Remember how the universal approximation theorems tell us that neural networks can basically approximate any function. \n", "In this exercise we will use the ```approximate_NN()``` to test this theorem. You can supply custom mathematical function to approximate using lambdas such as the call:\n", "\n", "```python\n", "approximate_NN(lambda x: 3*(x**2) + 6, -10, 10, epochs=50, hidden_layers=1, neurons_per_layer=32)\n", "```\n", "\n", "will try to approximate the function $f(x):=3x^2 + 6$ within the interval $[-10;10]$ using 50 epochs for training a single layer of hidden neurons with 32 neurons each.\n", "\n", "I have also supplied you with some more complex functions (```target_poly```, ```target_poly7```, ```target_poly14```, ```mother_wavelet```).\n", "\n", "Try to answer these questions:\n", "* What is the effect of changing the number of neurons vs. the number of hidden layers?\n", "* What is the effect of having very simple vs. more and more complex functions with regards the the required neurons/layers?\n", "* Does more training (more epochs) automatically mean better result?\n", "* Can you find a function that we can not approximate?\n", "* What happens outside the interval we used for training (indicated by the red dashed lines in the plots)?   \n", "\n", "\n", "\n", "    "]}, {"cell_type": "markdown", "id": "f0b68982", "metadata": {}, "source": ["### Exercise 2:"]}, {"cell_type": "code", "execution_count": null, "id": "588d671c", "metadata": {}, "outputs": [], "source": []}], "metadata": {"celltoolbar": "Edit Metadata", "kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.12"}}, "nbformat": 4, "nbformat_minor": 5}